---
title: "ہفتہ 10-11: Vision-Language Integration اور Foundation Models"
sidebar_position: 14
---

# ہفتہ 10-11: Vision-Language Integration اور Foundation Models

یہ 2 ہفتے اہم ہیں کیونکہ یہ Vision-Language ماڈلز کے انضمام کا جائزہ لیتے ہیں جو جسمانی AI کے لیے بنیاد ہیں۔ آپ OpenVLA اور دیگر VLA ماڈلز کی تربیت اور نافذ کاری کے بارے میں سیکھیں گے۔

## سیکھنے کے اہداف

اس ہفتے کے اختتام تک، آپ درج ذیل کر سکیں گے:

- Vision-Language ماڈلز کے تصورات کو سمجھنا
- OpenVLA اور دیگر VLA ماڈلز کا استعمال
- VLA ماڈلز کی تربیت اور متناسب بنانا
- VLA ماڈلز کو روبوٹکس سسٹم میں ضم کرنا
- VLA ماڈلز کی کارکردگی کا جائزہ لینا

## Vision-Language ماڈلز کا تعارف

Vision-Language ماڈلز تصویر اور لفظی ان پٹ دونوں کو سمجھنے کے قابل ہیں۔ یہ اہم ہیں:

- **Perception**: تصویر کو سمجھنا اور تصویر میں اشیاء کی شناخت
- **Instruction Understanding**: لفظی حکم کو سمجھنا
- **Action Planning**: تصویر کی تفہیم اور لفظی حکم کی بنیاد پر ایکشن کی منصوبہ بندی

### CLIP اور دیگر ماڈلز

CLIP (Contrastive Language-Image Pre-training) ایک بنیادی Vision-Language ماڈل ہے:

```python
from transformers import CLIPProcessor, CLIPModel
import torch
import requests
from PIL import Image

# CLIP ماڈل لوڈ کریں
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# تصویر لوڈ کریں
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# لفظی حکم
texts = ["a photo of a cat", "a photo of a dog"]

# کلاس کی اسکور تیار کریں
inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)

with torch.no_grad():
    logits_per_image = model(**inputs).logits_per_image
    probs = logits_per_image.softmax(dim=1)

print(probs)  # [cat probability, dog probability]
```

### OpenVLA Maڈل

OpenVLA (Open Vision-Language-Action) ایک کھلی ذمہ داری ہے ماڈل جو تصویر، لفظی، اور ایکشن کو ضم کرتا ہے:

```python
import torch
from openvla import OpenVLA

# OpenVLA ماڈل لوڈ کریں
model = OpenVLA.from_pretrained("openvla/openvla-9b")

# تصویر اور لفظی حکم کو استعمال کرتے ہوئے ایکشن تیار کریں
def predict_robot_action(image, instruction):
    # پرومسیسنگ
    inputs = model.preprocess_inputs(image, instruction)
    
    # ایکشن تیار کریں
    action = model(inputs)
    
    return action.detach().numpy()

# مثال کا استعمال
image_path = "robot_scene.jpg"
instruction = "pick up the red cup on the table"

action = predict_robot_action(image_path, instruction)
print(f"Predicted action: {action}")
```

## VLA ماڈلز کی تربیت

VLA ماڈلز کی تربیت کے لیے ہمیں بصورت، لفظی، اور ایکشن ڈیٹا کی ضرورت ہوتی ہے:

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset

class VLADataset(Dataset):
    def __init__(self, image_paths, instructions, actions):
        self.image_paths = image_paths
        self.instructions = instructions
        self.actions = actions
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # تصویر کو لوڈ کریں اور پروسیس کریں
        image = self.load_and_process_image(self.image_paths[idx])
        
        # لفظی حکم
        instruction = self.instructions[idx]
        
        # ایکشن
        action = torch.tensor(self.actions[idx], dtype=torch.float32)
        
        return {
            'image': image,
            'instruction': instruction,
            'action': action
        }
    
    def load_and_process_image(self, path):
        from PIL import Image
        import torchvision.transforms as transforms
        
        image = Image.open(path).convert('RGB')
        
        preprocess = transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                std=[0.229, 0.224, 0.225]),
        ])
        
        return preprocess(image)

def train_vla_model(model, dataset, epochs=10, batch_size=4):
    from torch.utils.data import DataLoader
    
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        for batch in dataloader:
            optimizer.zero_grad()
            
            # ماڈل کے ذریعے فارورڈ پاس
            predicted_actions = model(batch['image'], batch['instruction'])
            
            # نقصان کا حساب
            loss = criterion(predicted_actions, batch['action'])
            
            # بیک ورڈ پاس
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')
```

## VLA ماڈلز کو نافذ کرنا

VLA ماڈلز کو ROS 2 میں نافذ کرنا:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import torch
from transformers import CLIPProcessor, CLIPModel

class VLAModelNode(Node):
    def __init__(self):
        super().__init__('vla_model_node')
        
        # OpenVLA ماڈل کو لوڈ کریں
        self.vla_model = self.load_vla_model()
        self.cv_bridge = CvBridge()
        
        # سبسکرائبرز
        self.image_subscription = self.create_subscription(
            Image,
            'camera/image_raw',
            self.image_callback,
            10
        )
        
        self.instruction_subscription = self.create_subscription(
            String,
            'robot_instructions',
            self.instruction_callback,
            10
        )
        
        # پبلشر
        self.action_publisher = self.create_publisher(Twist, 'predicted_action', 10)
        
        # آخری تصویر اور حکم کو محفوظ کریں
        self.last_image = None
        self.last_instruction = None

    def load_vla_model(self):
        """VLA ماڈل کو لوڈ کریں"""
        # اصلی VLA ماڈل کو یہاں لوڈ کریں
        # اس کے لیے، ہم CLIP کو ایک نمونے کے طور پر استعمال کر سکتے ہیں
        model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        return model
    
    def image_callback(self, msg):
        """تصویر کی موصولی کی کال بیک"""
        try:
            # ROS تصویر کو OpenCV کے فارمیٹ میں تبدیل کریں
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")
            
            # تصویر کو آخری تصویر کے طور پر محفوظ کریں
            self.last_image = cv_image
            
            # اگر آخری حکم بھی دستیاب ہو تو کارروائی تیار کریں
            if self.last_instruction is not None:
                self.generate_and_publish_action()
                
        except Exception as e:
            self.get_logger().error(f'Error processing image: {str(e)}')
    
    def instruction_callback(self, msg):
        """لفظی حکم کی موصولی کی کال بیک"""
        # حکم کو آخری حکم کے طور پر محفوظ کریں
        self.last_instruction = msg.data
        
        # اگر آخری تصویر بھی دستیاب ہو تو کارروائی تیار کریں
        if self.last_image is not None:
            self.generate_and_publish_action()
    
    def generate_and_publish_action(self):
        """آخری تصویر اور حکم کے لیے کارروائی تیار کریں اور شائع کریں"""
        if self.last_image is None or self.last_instruction is None:
            self.get_logger().warn('Insufficient data to generate action')
            return
        
        try:
            # VLA ماڈل سے کارروائی تیار کریں
            action = self.predict_action(self.last_image, self.last_instruction)
            
            # کارروائی کو شائع کریں
            action_msg = self.convert_action_to_twist(action)
            self.action_publisher.publish(action_msg)
            
            self.get_logger().info(f'Published action: {action}')
            
        except Exception as e:
            self.get_logger().error(f'Error generating action: {str(e)}')
    
    def predict_action(self, image, instruction):
        """تصویر اور حکم سے کارروائی کی پیشن گوئی کریں"""
        # اس کو اصلی VLA ماڈل کے ساتھ تبدیل کیا جائے گا
        # یہ صرف ایک مثال ہے
        return [0.1, 0.0, 0.0, 0.0, 0.0, 0.1]  # 6-DOF action