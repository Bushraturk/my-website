"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4870],{5074:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"vla/week13","title":"Week 13 - Course Synthesis and Capstone Project","description":"In this final week of the Physical AI & Humanoid Robotics course, you\'ll synthesize everything you\'ve learned across all modules to implement a comprehensive AI-powered robotic system. This capstone project will integrate ROS 2, simulation environments, AI perception and reasoning, and vision-language-action capabilities into a complete embodied AI system.","source":"@site/docs/vla/week13.md","sourceDirName":"vla","slug":"/vla/week13","permalink":"/vla/week13","draft":false,"unlisted":false,"editUrl":"https://github.com/Bushraturk/Physical-AI-Book/edit/main/docs/vla/week13.md","tags":[],"version":"current","sidebarPosition":19,"frontMatter":{"title":"Week 13 - Course Synthesis and Capstone Project","sidebar_position":19},"sidebar":"textbookSidebar","previous":{"title":"Week 12 - Action Planning with Large Language Models","permalink":"/vla/week12"},"next":{"title":"Lab Exercise - Vision-Language-Action Integration","permalink":"/vla/lab-exercises/lab1"}}');var r=t(4848),s=t(8453);const o={title:"Week 13 - Course Synthesis and Capstone Project",sidebar_position:19},a="Week 13: Course Synthesis and Capstone Project",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Capstone Project: Autonomous Task Execution with Natural Language Interaction",id:"capstone-project-autonomous-task-execution-with-natural-language-interaction",level:2},{value:"Project Requirements",id:"project-requirements",level:3},{value:"Example Project Scenarios",id:"example-project-scenarios",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"Component Integration",id:"component-integration",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: System Design (Day 1)",id:"step-1-system-design-day-1",level:3},{value:"Step 2: Environment Setup (Day 1)",id:"step-2-environment-setup-day-1",level:3},{value:"Step 3: Language-to-Action Pipeline (Days 1-2)",id:"step-3-language-to-action-pipeline-days-1-2",level:3},{value:"Step 4: Project Launch File",id:"step-4-project-launch-file",level:3},{value:"Step 5: Testing and Evaluation (Days 3-4)",id:"step-5-testing-and-evaluation-days-3-4",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Technical Implementation (40 points)",id:"technical-implementation-40-points",level:3},{value:"System Design (25 points)",id:"system-design-25-points",level:3},{value:"Performance (20 points)",id:"performance-20-points",level:3},{value:"Innovation and Creativity (15 points)",id:"innovation-and-creativity-15-points",level:3},{value:"Presentation Requirements",id:"presentation-requirements",level:2},{value:"Project Report",id:"project-report",level:2},{value:"Code Repository Structure",id:"code-repository-structure",level:2},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Integration Challenges",id:"integration-challenges",level:3},{value:"Debugging Strategies",id:"debugging-strategies",level:3},{value:"Advanced Extensions (Optional)",id:"advanced-extensions-optional",level:2},{value:"Resources",id:"resources",level:2},{value:"Summary",id:"summary",level:2},{value:"Navigation",id:"navigation",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"week-13-course-synthesis-and-capstone-project",children:"Week 13: Course Synthesis and Capstone Project"})}),"\n",(0,r.jsx)(e.p,{children:"In this final week of the Physical AI & Humanoid Robotics course, you'll synthesize everything you've learned across all modules to implement a comprehensive AI-powered robotic system. This capstone project will integrate ROS 2, simulation environments, AI perception and reasoning, and vision-language-action capabilities into a complete embodied AI system."}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Integrate all components learned throughout the course into a unified system"}),"\n",(0,r.jsx)(e.li,{children:"Design and implement a complete AI-robot system with perception, planning, and control"}),"\n",(0,r.jsx)(e.li,{children:"Deploy a vision-language-action pipeline on a physical or simulated robot platform"}),"\n",(0,r.jsx)(e.li,{children:"Evaluate the performance of your integrated system against specific benchmarks"}),"\n",(0,r.jsx)(e.li,{children:"Document and present your project as a complete system"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"capstone-project-autonomous-task-execution-with-natural-language-interaction",children:"Capstone Project: Autonomous Task Execution with Natural Language Interaction"}),"\n",(0,r.jsx)(e.p,{children:"For your capstone project, you'll build a complete system that demonstrates the integration of all course modules:"}),"\n",(0,r.jsx)(e.h3,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,r.jsx)(e.p,{children:"Your system must:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Accept Natural Language Commands"}),": Process high-level tasks expressed in natural language"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Perceive the Environment"}),": Use vision and other sensors to understand the current state"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Plan Actions"}),": Generate a sequence of actions to accomplish the requested task"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Execute Actions"}),": Control the robot to perform the planned actions in simulation or on a physical platform"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Handle Unexpected Situations"}),": Adapt to changes in the environment or initial plan failures"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"example-project-scenarios",children:"Example Project Scenarios"}),"\n",(0,r.jsx)(e.p,{children:"Choose one of the following scenarios or propose your own with instructor approval:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Warehouse Assistant"}),': Robot that understands commands like "Retrieve the red box from shelf A and place it in the blue container" in a simulated warehouse environment']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Social Robot Helper"}),': Robot that assists with household tasks like "Bring me the water bottle from the kitchen table"']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Laboratory Assistant"}),": Robot that performs scientific tasks like \"Move sample A from rack 1 to microscope, acquire image, and store in folder 'analysis'\""]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Educational Tutor"}),": Robot that demonstrates simple physics concepts based on verbal descriptions"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,r.jsx)(e.p,{children:"Your system will integrate components from all previous modules:"}),"\n",(0,r.jsx)(e.mermaid,{value:"graph TB\r\n    A[Natural Language Command] --\x3e B[Large Language Model]\r\n    B --\x3e C[Vision-Language-ACTION Model]\r\n    C --\x3e D[Task Planner]\r\n    D --\x3e E[ROS 2 Action Server]\r\n    E --\x3e F[Simulation Environment<br/>or Physical Robot]\r\n    F --\x3e G[Perception System]\r\n    G --\x3e H[NVIDIA Isaac Perception]\r\n    H --\x3e I[VSLAM/Mapping]\r\n    I --\x3e J[State Monitor]\r\n    J --\x3e K[Control System]\r\n    K --\x3e F\r\n    D --\x3e L[Navigation System]\r\n    L --\x3e M[Gazebo/Unity Simulation]\r\n    M --\x3e F\r\n    G --\x3e J\r\n    L --\x3e K\r\n    F --\x3e N[Success/Failure Feedback]\r\n    N --\x3e B\r\n    C --\x3e O[Knowledge Graph]\r\n    O --\x3e B"}),"\n",(0,r.jsx)(e.h3,{id:"component-integration",children:"Component Integration"}),"\n",(0,r.jsx)(e.p,{children:"Your system should demonstrate integration across:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Module 1 (ROS 2)"}),": All communication between system components should use ROS 2 topics, services, and actions"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Module 2 (Gazebo/Unity)"}),": Use simulation for testing and development, with sim-to-real transfer"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Module 3 (NVIDIA Isaac)"}),": Implement perception systems using Isaac's AI capabilities"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Module 4 (VLA)"}),": Process natural language commands and generate appropriate actions"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,r.jsx)(e.h3,{id:"step-1-system-design-day-1",children:"Step 1: System Design (Day 1)"}),"\n",(0,r.jsx)(e.p,{children:"Create a design document for your project that includes:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"System architecture diagram showing component interactions"}),"\n",(0,r.jsx)(e.li,{children:"ROS 2 node topology with message topics and services"}),"\n",(0,r.jsx)(e.li,{children:"Data flow from language input to action execution"}),"\n",(0,r.jsx)(e.li,{children:"Error handling and fallback strategies"}),"\n",(0,r.jsx)(e.li,{children:"Component interfaces and responsibilities"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"step-2-environment-setup-day-1",children:"Step 2: Environment Setup (Day 1)"}),"\n",(0,r.jsx)(e.p,{children:"Set up the integrated environment:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Set up workspace for the integrated system\r\nmkdir -p ~/capstone_ws/src\r\ncd ~/capstone_ws/src\r\n\r\n# Clone necessary packages\r\ngit clone https://github.com/ros-planning/navigation2.git\r\ngit clone https://github.com/protocolbuffers/paligemma-description.git  # For vision-language models\r\n\r\n# Build the workspace\r\ncd ~/capstone_ws\r\ncolcon build --symlink-install\r\nsource install/setup.bash\n"})}),"\n",(0,r.jsx)(e.h3,{id:"step-3-language-to-action-pipeline-days-1-2",children:"Step 3: Language-to-Action Pipeline (Days 1-2)"}),"\n",(0,r.jsx)(e.p,{children:"Implement the language understanding and action planning component:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# capstone_project/capstone_project/language_action_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\r\nfrom sensor_msgs.msg import Image\r\nimport openai\r\nimport yaml\r\nimport json\r\nfrom typing import Dict, List, Any\r\n\r\n\r\nclass CapstoneLanguageActionNode(Node):\r\n    """\r\n    Capstone integration node that connects language understanding to robot action.\r\n    Integrates all course modules: ROS 2, Gazebo/Unity, NVIDIA Isaac, and VLA.\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'capstone_integration_node\')\r\n        \r\n        # Initialize components from all modules\r\n        self.initialize_ros_components()\r\n        self.initialize_perception_system()\r\n        self.initialize_language_model()\r\n        self.initialize_navigation_system()\r\n        \r\n        # Set up communication\r\n        self.command_subscriber = self.create_subscription(\r\n            String,\r\n            \'/natural_language_command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n        self.status_publisher = self.create_publisher(\r\n            String,\r\n            \'/system_status\',\r\n            10\r\n        )\r\n        \r\n        self.get_logger().info("Capstone Integration Node Initialized")\r\n    \r\n    def initialize_ros_components(self):\r\n        """Initialize ROS 2 communication components from Module 1"""\r\n        self.get_logger().info("Initializing ROS 2 components...")\r\n        # Initialize action clients, services, and publishers for robot control\r\n        # This would include navigation goals, manipulation actions, etc.\r\n        \r\n    def initialize_perception_system(self):\r\n        """Initialize perception using NVIDIA Isaac (Module 3)"""\r\n        self.get_logger().info("Initializing perception system...")\r\n        # Initialize object detection, SLAM, and other Isaac-based perception\r\n        # components for environmental understanding\r\n        \r\n    def initialize_language_model(self):\r\n        """Initialize VLA model for command interpretation (Module 4)"""\r\n        self.get_logger().info("Initializing language model...")\r\n        # Initialize your chosen VLA model (either use a pre-trained one \r\n        # or a simple rule-based system for demonstration)\r\n        \r\n    def initialize_navigation_system(self):\r\n        """Initialize navigation in simulation environment (Module 2)"""\r\n        self.get_logger().info("Initializing navigation system...")\r\n        # Set up navigation stack for movement in the environment\r\n        \r\n    def command_callback(self, msg):\r\n        """Process natural language command and execute robotic task"""\r\n        command = msg.data\r\n        self.get_logger().info(f"Received command: \'{command}\'")\r\n        \r\n        try:\r\n            # Step 1: Use LLM to interpret the command\r\n            action_plan = self.interpret_command(command)\r\n            \r\n            # Step 2: Check if task is feasible with current perception\r\n            if not self.validate_plan_against_perception(action_plan):\r\n                self.publish_error("Task not feasible with current environment state")\r\n                return\r\n            \r\n            # Step 3: Execute the action plan\r\n            success = self.execute_action_plan(action_plan)\r\n            \r\n            # Step 4: Report outcome\r\n            if success:\r\n                self.publish_success(f"Task completed: {command}")\r\n            else:\r\n                self.publish_error(f"Task failed: {command}")\r\n                \r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing command: {e}")\r\n            self.publish_error(f"Error processing command: {str(e)}")\r\n    \r\n    def interpret_command(self, command: str) -> List[Dict[str, Any]]:\r\n        """\r\n        Interpret natural language command using VLA model.\r\n        \r\n        Args:\r\n            command: Natural language command from user\r\n            \r\n        Returns:\r\n            List of actions to execute\r\n        """\r\n        # This would typically use a VLA model to interpret the command\r\n        # For this example, we\'ll use a simple rule-based interpreter\r\n        # that demonstrates the concept\r\n        \r\n        command_lower = command.lower()\r\n        \r\n        actions = []\r\n        \r\n        if "go to" in command_lower or "move to" in command_lower:\r\n            # Extract location information\r\n            if "kitchen" in command_lower:\r\n                actions.append({\r\n                    "action": "navigate_to",\r\n                    "target_location": "kitchen_waypoint"\r\n                })\r\n            elif "table" in command_lower:\r\n                actions.append({\r\n                    "action": "navigate_to", \r\n                    "target_location": "table_waypoint"\r\n                })\r\n                \r\n        if "pick up" in command_lower or "grasp" in command_lower:\r\n            # Extract object information\r\n            if "water bottle" in command_lower:\r\n                actions.append({\r\n                    "action": "pick_object",\r\n                    "object_name": "water_bottle"\r\n                })\r\n                \r\n        if "place" in command_lower or "put" in command_lower:\r\n            actions.append({\r\n                "action": "place_object",\r\n                "object_name": "water_bottle",\r\n                "target_location": "counter"\r\n            })\r\n            \r\n        return actions\r\n    \r\n    def validate_plan_against_perception(self, action_plan: List[Dict[str, Any]]) -> bool:\r\n        """\r\n        Check if the action plan is feasible with current environmental state.\r\n        \r\n        Args:\r\n            action_plan: Planned actions to execute\r\n            \r\n        Returns:\r\n            True if plan is feasible, False otherwise\r\n        """\r\n        # Use perception system to validate plan feasibility\r\n        # This is where Isaac perception comes into play\r\n        self.get_logger().info("Validating plan against current perception...")\r\n        \r\n        # For simplicity, just return True in this example\r\n        # In practice, check if objects are where expected, etc.\r\n        return True\r\n    \r\n    def execute_action_plan(self, action_plan: List[Dict[str, Any]]) -> bool:\r\n        """\r\n        Execute a sequence of planned actions.\r\n        \r\n        Args:\r\n            action_plan: List of actions to execute\r\n            \r\n        Returns:\r\n            True if all actions completed successfully, False otherwise\r\n        """\r\n        for i, action in enumerate(action_plan):\r\n            self.get_logger().info(f"Executing action {i+1}/{len(action_plan)}: {action}")\r\n            \r\n            success = self.execute_single_action(action)\r\n            \r\n            if not success:\r\n                self.get_logger().error(f"Action failed: {action}")\r\n                return False\r\n                \r\n        return True\r\n    \r\n    def execute_single_action(self, action: Dict[str, Any]) -> bool:\r\n        """Execute a single robot action."""\r\n        action_type = action.get("action", "")\r\n        \r\n        if action_type == "navigate_to":\r\n            return self.execute_navigation(action)\r\n        elif action_type == "pick_object":\r\n            return self.execute_manipulation_pick(action)\r\n        elif action_type == "place_object":\r\n            return self.execute_manipulation_place(action)\r\n        else:\r\n            self.get_logger().error(f"Unknown action type: {action_type}")\r\n            return False\r\n    \r\n    def execute_navigation(self, action: Dict[str, Any]) -> bool:\r\n        """Execute navigation action using Gazebo/Unity simulation."""\r\n        target_location = action.get("target_location", "")\r\n        \r\n        self.get_logger().info(f"Navigating to: {target_location}")\r\n        \r\n        # In a real implementation, this would send navigation goals\r\n        # and monitor execution progress\r\n        # For simulation, we\'d use navigation2 stack\r\n        \r\n        # Simulate navigation completion\r\n        import time\r\n        time.sleep(2)  # Simulate navigation time\r\n        \r\n        self.get_logger().info("Navigation completed")\r\n        return True\r\n    \r\n    def execute_manipulation_pick(self, action: Dict[str, Any]) -> bool:\r\n        """Execute pick action using Isaac perception and control."""\r\n        object_name = action.get("object_name", "")\r\n        \r\n        self.get_logger().info(f"Picking up: {object_name}")\r\n        \r\n        # In a real implementation, this would use perception to locate\r\n        # the object and plan a grasping motion\r\n        # Use Isaac perception to identify object location\r\n        \r\n        # Simulate pick completion\r\n        import time\r\n        time.sleep(3)  # Simulate pick action time\r\n        \r\n        self.get_logger().info("Pick completed")\r\n        return True\r\n    \r\n    def execute_manipulation_place(self, action: Dict[str, Any]) -> bool:\r\n        """Execute place action using Isaac perception and control."""\r\n        object_name = action.get("object_name", "")\r\n        location = action.get("target_location", "")\r\n        \r\n        self.get_logger().info(f"Placing: {object_name} at {location}")\r\n        \r\n        # In a real implementation, this would use perception to locate\r\n        # the placement target and plan a placing motion\r\n        \r\n        # Simulate place completion\r\n        import time\r\n        time.sleep(2)  # Simulate place action time\r\n        \r\n        self.get_logger().info("Place completed")\r\n        return True\r\n    \r\n    def publish_success(self, message: str):\r\n        """Publish success message."""\r\n        status_msg = String()\r\n        status_msg.data = f"SUCCESS: {message}"\r\n        self.status_publisher.publish(status_msg)\r\n    \r\n    def publish_error(self, message: str):\r\n        """Publish error message."""\r\n        status_msg = String()\r\n        status_msg.data = f"ERROR: {message}"\r\n        self.status_publisher.publish(status_msg)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    \r\n    capstone_node = CapstoneLanguageActionNode()\r\n    \r\n    try:\r\n        rclpy.spin(capstone_node)\r\n    except KeyboardInterrupt:\r\n        capstone_node.get_logger().info("Shutting down capstone integration node...")\r\n    finally:\r\n        capstone_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"step-4-project-launch-file",children:"Step 4: Project Launch File"}),"\n",(0,r.jsx)(e.p,{children:"Create a launch file to bring up your integrated system:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:"\x3c!-- capstone_project/launch/capstone_system.launch.py --\x3e\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom ament_index_python.packages import get_package_share_directory\r\nimport os\r\n\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\r\n    \r\n    # Capstone integration node\r\n    capstone_integration_node = Node(\r\n        package='capstone_project',\r\n        executable='capstone_integration_node',\r\n        name='capstone_integration_node',\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time}\r\n        ],\r\n        remappings=[\r\n            ('/natural_language_command', '/user_commands'),\r\n            ('/system_status', '/capstone/status')\r\n        ],\r\n        output='screen'\r\n    )\r\n    \r\n    # Perception node using Isaac packages\r\n    perception_node = Node(\r\n        package='isaac_ros_pointcloud_utils',\r\n        executable='pointcloud_to_laserscan_node',\r\n        name='perception_node',\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time},\r\n            {'min_height': 0.1},\r\n            {'max_height': 2.0},\r\n            {'scan_height': 1.0}\r\n        ]\r\n    )\r\n    \r\n    # Navigation node\r\n    navigation_node = Node(\r\n        package='nav2_planner',\r\n        executable='nav2_planner',\r\n        name='nav2_planner_server',\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time}\r\n        ]\r\n    )\r\n    \r\n    return LaunchDescription([\r\n        capstone_integration_node,\r\n        perception_node,\r\n        navigation_node\r\n    ])\n"})}),"\n",(0,r.jsx)(e.h3,{id:"step-5-testing-and-evaluation-days-3-4",children:"Step 5: Testing and Evaluation (Days 3-4)"}),"\n",(0,r.jsx)(e.p,{children:"Develop tests for your integrated system:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unit Tests"}),": Test individual components in isolation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Integration Tests"}),": Test how components work together"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"System Tests"}),": Test end-to-end functionality"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Performance Tests"}),": Measure execution time, success rate, etc."]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# capstone_project/test/test_capstone_integration.py\r\nimport unittest\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom capstone_project.language_action_node import CapstoneLanguageActionNode\r\nimport time\r\n\r\n\r\nclass TestCapstoneIntegration(unittest.TestCase):\r\n    \r\n    def setUp(self):\r\n        rclpy.init()\r\n        self.node = CapstoneLanguageActionNode()\r\n        \r\n    def tearDown(self):\r\n        self.node.destroy_node()\r\n        rclpy.shutdown()\r\n    \r\n    def test_command_interpretation(self):\r\n        """Test that natural language commands are correctly interpreted."""\r\n        command = "Go to the kitchen and pick up the water bottle"\r\n        action_plan = self.node.interpret_command(command)\r\n        \r\n        # Check that navigation and manipulation actions are generated\r\n        navigation_actions = [a for a in action_plan if a.get("action") == "navigate_to"]\r\n        manipulation_actions = [a for a in action_plan if "object" in str(a)]\r\n        \r\n        self.assertTrue(len(navigation_actions) > 0, "Should generate navigation actions")\r\n        self.assertTrue(len(manipulation_actions) > 0, "Should generate manipulation actions")\r\n    \r\n    def test_action_execution_simulation(self):\r\n        """Test that actions can be executed in simulation."""\r\n        # Create a simple action plan\r\n        action_plan = [\r\n            {"action": "navigate_to", "target_location": "test_location"}\r\n        ]\r\n        \r\n        # Execute the plan (in simulation, this should complete without errors)\r\n        success = self.node.execute_action_plan(action_plan)\r\n        \r\n        self.assertTrue(success, "Action plan should execute successfully in simulation")\r\n\r\n\r\ndef test_suite():\r\n    """Create a test suite for the capstone project."""\r\n    suite = unittest.TestSuite()\r\n    \r\n    # Add tests to suite\r\n    suite.addTest(TestCapstoneIntegration(\'test_command_interpretation\'))\r\n    suite.addTest(TestCapstoneIntegration(\'test_action_execution_simulation\'))\r\n    \r\n    return suite\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    runner = unittest.TextTestRunner(verbosity=2)\r\n    runner.run(test_suite())\n'})}),"\n",(0,r.jsx)(e.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,r.jsx)(e.p,{children:"Your capstone project will be evaluated on:"}),"\n",(0,r.jsx)(e.h3,{id:"technical-implementation-40-points",children:"Technical Implementation (40 points)"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Module Integration (20 points)"}),": Proper integration of components from all 4 modules"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Functionality (20 points)"}),": System performs as expected for the chosen scenario"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"system-design-25-points",children:"System Design (25 points)"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Architecture (10 points)"}),": Well-designed system architecture with clear component separation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Documentation (10 points)"}),": Clear documentation explaining design decisions and implementation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Code Quality (5 points)"}),": Clean, well-commented, and maintainable code"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"performance-20-points",children:"Performance (20 points)"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Task Success Rate (10 points)"}),": Percentage of tasks completed successfully"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Efficiency (10 points)"}),": Execution time and resource utilization efficiency"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"innovation-and-creativity-15-points",children:"Innovation and Creativity (15 points)"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Originality (8 points)"}),": Creative approach to solving the problem"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Advanced Features (7 points)"}),": Implementation of advanced features beyond basic requirements"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"presentation-requirements",children:"Presentation Requirements"}),"\n",(0,r.jsx)(e.p,{children:"Prepare a 10-minute presentation that covers:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Problem Statement"})," (1 minute)"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"What task does your system perform?"}),"\n",(0,r.jsx)(e.li,{children:"Why is it challenging?"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"System Design"})," (3 minutes)"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Architecture diagram showing module integration"}),"\n",(0,r.jsx)(e.li,{children:"Key components and their roles"}),"\n",(0,r.jsx)(e.li,{children:"How components communicate"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Implementation"})," (3 minutes)"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Key technical challenges overcome"}),"\n",(0,r.jsx)(e.li,{children:"Code snippets demonstrating critical functionality"}),"\n",(0,r.jsx)(e.li,{children:"Demonstration (video or live if possible)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Results and Evaluation"})," (2 minutes)"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Performance metrics and success stories"}),"\n",(0,r.jsx)(e.li,{children:"Limitations and failure cases"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Lessons Learned"})," (1 minute)"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"What worked well, what didn't"}),"\n",(0,r.jsx)(e.li,{children:"Ideas for future improvement"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"project-report",children:"Project Report"}),"\n",(0,r.jsx)(e.p,{children:"Submit a written report (4-6 pages) including:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Abstract"}),": Summary of your system and results"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Introduction"}),": Problem motivation and approach"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Related Work"}),": Connection to relevant literature and course content"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"System Design"}),": Complete architecture and component descriptions"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Implementation"}),": Technical details of your approach"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Evaluation"}),": Experimental setup, results, and analysis"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Discussion"}),": Lessons learned and future work"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"References"}),": Academic sources and course materials referenced"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"code-repository-structure",children:"Code Repository Structure"}),"\n",(0,r.jsx)(e.p,{children:"Organize your project code as follows:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"capstone_project/\r\n\u251c\u2500\u2500 README.md                    # Project overview and setup instructions\r\n\u251c\u2500\u2500 src/\r\n\u2502   \u2514\u2500\u2500 capstone_project/\r\n\u2502       \u251c\u2500\u2500 __init__.py\r\n\u2502       \u251c\u2500\u2500 language_action_node.py    # Main integration node\r\n\u2502       \u251c\u2500\u2500 perception_handler.py      # Isaac-based perception\r\n\u2502       \u251c\u2500\u2500 navigation_manager.py      # Navigation component\r\n\u2502       \u2514\u2500\u2500 utils/\r\n\u2502           \u251c\u2500\u2500 command_parser.py      # Natural language processing\r\n\u2502           \u2514\u2500\u2500 system_monitor.py      # Status and error handling\r\n\u251c\u2500\u2500 launch/\r\n\u2502   \u2514\u2500\u2500 capstone_system.launch.py     # Launch file for complete system\r\n\u251c\u2500\u2500 config/\r\n\u2502   \u2514\u2500\u2500 capstone_params.yaml          # Configuration parameters\r\n\u251c\u2500\u2500 test/\r\n\u2502   \u2514\u2500\u2500 test_capstone_integration.py  # Unit and integration tests\r\n\u251c\u2500\u2500 docs/\r\n\u2502   \u251c\u2500\u2500 architecture_diagram.png       # System architecture diagram\r\n\u2502   \u2514\u2500\u2500 user_guide.md                 # User documentation\r\n\u2514\u2500\u2500 package.xml                       # ROS 2 package definition\n"})}),"\n",(0,r.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,r.jsx)(e.h3,{id:"integration-challenges",children:"Integration Challenges"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Timing Issues"}),": Ensure components have time to initialize before communication begins"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Coordinate Frames"}),": Verify all components use consistent coordinate frame conventions"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Message Types"}),": Ensure sender and receiver agree on message types and structures"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Performance"}),": Monitor CPU and memory usage across all components"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"debugging-strategies",children:"Debugging Strategies"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Logging"}),": Implement comprehensive logging to trace data flow"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"RViz Visualization"}),": Use RViz to visualize robot state, navigation plans, and perception results"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Topic Echo"}),": Use ",(0,r.jsx)(e.code,{children:"ros2 topic echo"})," to verify message content and timing"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simulation Testing"}),": Thoroughly test in simulation before attempting physical robot deployment"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"advanced-extensions-optional",children:"Advanced Extensions (Optional)"}),"\n",(0,r.jsx)(e.p,{children:"For additional credit, consider implementing one or more of these advanced features:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Multi-Modal Grounding"}),": Integrate speech recognition for spoken commands"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Dynamic Replanning"}),": Adjust plans when the environment changes during execution"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Collaborative Robotics"}),": Coordinate with other robots or humans during task execution"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Learning from Demonstration"}),": Allow the system to learn new tasks from human demonstrations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Long-term Autonomy"}),": Handle extended operations with periodic recharging and maintenance"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"resources",children:"Resources"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://docs.ros.org/",children:"ROS 2 Documentation"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://nvidia-isaac-ros.github.io/",children:"NVIDIA Isaac ROS Documentation"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"http://gazebosim.org/docs/harmonic",children:"Gazebo Harmonic Documentation"})}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.a,{href:"https://vila-project.github.io/",children:"OpenVLA Project"})}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"In this capstone week, you've synthesized all the knowledge and skills from the entire course:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ROS 2 Fundamentals"}),": Applied communication patterns learned in Module 1"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simulation Environments"}),": Used Gazebo/Unity skills from Module 2 to test and develop"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"AI Perception"}),": Implemented NVIDIA Isaac perception from Module 3"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Vision-Language-Action"}),": Integrated VLA capabilities from Module 4"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"You've built a complete embodied AI system that demonstrates how all these components work together to create intelligent robotic agents capable of understanding natural language commands and executing them in the physical world."}),"\n",(0,r.jsx)(e.h2,{id:"navigation",children:"Navigation"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.a,{href:"/vla/week12",children:"\u2190 Previous: Week 12: Action Planning with LLMs"})," | ",(0,r.jsx)(e.a,{href:"/conclusion",children:"Course Conclusion"})," | ",(0,r.jsx)(e.a,{href:"/vla/intro",children:"Module Home"})]}),"\n",(0,r.jsxs)(e.p,{children:["Continue to the ",(0,r.jsx)(e.a,{href:"/conclusion",children:"Course Conclusion"})," to review everything you've learned and your journey through Physical AI & Humanoid Robotics."]})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>a});var i=t(6540);const r={},s=i.createContext(r);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);