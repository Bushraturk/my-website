"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6670],{3411:(e,n,r)=>{r.d(n,{A:()=>i});const i="data:image/png;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNDAwIj48cmVjdCB3aWR0aD0iODAwIiBoZWlnaHQ9IjQwMCIgZmlsbD0iI2YwZjBmMCIvPjx0ZXh0IHg9IjQwMCIgeT0iMjAwIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iMjAiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiMzMzMiPlZTTEFNIEFyY2hpdGVjdHVyZSBEaWFncmFtPC90ZXh0Pjx0ZXh0IHg9IjQwMCIgeT0iMjMwIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iMTYiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiM2NjYiPkZlYXR1cmUgRGV0ZWN0aW9uLCBNb3Rpb24gRXN0aW1hdGlvbiwgTWFwIEJ1aWxkaW5nPC90ZXh0Pjwvc3ZnPg=="},3547:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"nvidia-isaac/week7-8","title":"Week 7-8 - Perception and VSLAM with NVIDIA Isaac","description":"In these two weeks, we\'ll dive deep into perception systems powered by NVIDIA Isaac platform, focusing on spatial AI and Visual Simultaneous Localization and Mapping (VSLAM). You\'ll learn to implement perception algorithms that allow robots to understand their environment and navigate autonomously.","source":"@site/docs/nvidia-isaac/week7-8.md","sourceDirName":"nvidia-isaac","slug":"/nvidia-isaac/week7-8","permalink":"/nvidia-isaac/week7-8","draft":false,"unlisted":false,"editUrl":"https://github.com/Bushraturk/Physical-AI-Book/edit/main/docs/nvidia-isaac/week7-8.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Week 7-8 - Perception and VSLAM with NVIDIA Isaac","sidebar_position":10},"sidebar":"textbookSidebar","previous":{"title":"Introduction to NVIDIA Isaac - AI-Robot Brains and Perception","permalink":"/nvidia-isaac/intro"},"next":{"title":"Week 9 - AI-Robot Brain Integration and Reinforcement Learning","permalink":"/nvidia-isaac/week9"}}');var s=r(4848),a=r(8453);const t={title:"Week 7-8 - Perception and VSLAM with NVIDIA Isaac",sidebar_position:10},o="Week 7-8: Perception and VSLAM with NVIDIA Isaac",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to NVIDIA Isaac Perception",id:"introduction-to-nvidia-isaac-perception",level:2},{value:"Key Perception Components",id:"key-perception-components",level:3},{value:"Setting Up Isaac Perception",id:"setting-up-isaac-perception",level:2},{value:"Perception Nodes Architecture",id:"perception-nodes-architecture",level:3},{value:"Implementing Visual SLAM",id:"implementing-visual-slam",level:2},{value:"Isaac Sim VSLAM Example",id:"isaac-sim-vslam-example",level:3},{value:"Sensor Fusion with Isaac",id:"sensor-fusion-with-isaac",level:2},{value:"Multi-sensor Data Integration",id:"multi-sensor-data-integration",level:3},{value:"Isaac Perception Pipeline",id:"isaac-perception-pipeline",level:2},{value:"Practical Application",id:"practical-application",level:2},{value:"Lab Exercise Preview",id:"lab-exercise-preview",level:2},{value:"Summary",id:"summary",level:2},{value:"Navigation",id:"navigation",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-7-8-perception-and-vslam-with-nvidia-isaac",children:"Week 7-8: Perception and VSLAM with NVIDIA Isaac"})}),"\n",(0,s.jsx)(n.p,{children:"In these two weeks, we'll dive deep into perception systems powered by NVIDIA Isaac platform, focusing on spatial AI and Visual Simultaneous Localization and Mapping (VSLAM). You'll learn to implement perception algorithms that allow robots to understand their environment and navigate autonomously."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement perception algorithms using Isaac ROS packages"}),"\n",(0,s.jsx)(n.li,{children:"Understand and deploy VSLAM systems for robot navigation"}),"\n",(0,s.jsx)(n.li,{children:"Process sensor data with CUDA-accelerated libraries"}),"\n",(0,s.jsx)(n.li,{children:"Integrate perception systems with other ROS 2 nodes"}),"\n",(0,s.jsx)(n.li,{children:"Perform sensor fusion for enhanced environmental understanding"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-nvidia-isaac-perception",children:"Introduction to NVIDIA Isaac Perception"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Isaac Perception Pipeline",src:r(9161).A+"",width:"800",height:"600"})}),"\n",(0,s.jsx)(n.p,{children:"NVIDIA Isaac provides a comprehensive suite of perception tools built on top of NVIDIA's GPU-accelerated computing platform. The Isaac perception stack includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS"}),": ROS 2 packages optimized for NVIDIA hardware"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac SIM"}),": High-fidelity simulation environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep Learning Libraries"}),": CUDA-accelerated computer vision and neural networks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Processing Pipelines"}),": Optimized pipelines for sensor fusion"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-perception-components",children:"Key Perception Components"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual SLAM"}),": Map building and self-localization using cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep Learning-Based Perception"}),": Object detection, segmentation, classification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining multiple sensors for robust perception"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"3D Reconstruction"}),": Building 3D representations of the environment"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"setting-up-isaac-perception",children:"Setting Up Isaac Perception"}),"\n",(0,s.jsx)(n.p,{children:"First, let's install the necessary Isaac ROS packages:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Add NVIDIA Isaac repository\r\nsudo apt update && sudo apt install wget gnupg\r\nwget https://repo.download.nvidia.com/jetson-agx-xavier/jp50/GPGKEY\r\nsudo apt-key add GPGKEY\r\necho "deb https://repo.download.nvidia.com/jetson-agx-xavier/jp50 main" | sudo tee /etc/apt/sources.list.d/nvidia-l4t.list\r\nsudo apt update\r\n\r\n# Install Isaac ROS packages\r\nsudo apt install nvidia-jetpack\r\nsudo apt install nvidia-jetpack-all\n'})}),"\n",(0,s.jsx)(n.h3,{id:"perception-nodes-architecture",children:"Perception Nodes Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a perception pipeline that processes camera and lidar data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- robot_perception.urdf.xacro --\x3e\r\n<?xml version="1.0"?>\r\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="robot_with_perception">\r\n\r\n  <xacro:property name="M_PI" value="3.1415926535897931"/>\r\n\r\n  \x3c!-- Base link --\x3e\r\n  <link name="base_link">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.5 0.5 0.25"/>\r\n      </geometry>\r\n      <material name="blue">\r\n        <color rgba="0 0 1 0.8"/>\r\n      </material>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.5 0.5 0.25"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="1"/>\r\n      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  \x3c!-- RGB camera --\x3e\r\n  <link name="camera_link">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.05 0.1 0.05"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <box size="0.05 0.1 0.05"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="0.1"/>\r\n      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  \x3c!-- Camera joint --\x3e\r\n  <joint name="camera_joint" type="fixed">\r\n    <parent link="base_link"/>\r\n    <child link="camera_link"/>\r\n    <origin xyz="0.2 0 0.1" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  \x3c!-- LiDAR sensor --\x3e\r\n  <link name="lidar_link">\r\n    <visual>\r\n      <geometry>\r\n        <cylinder radius="0.05" length="0.05"/>\r\n      </geometry>\r\n    </visual>\r\n    <collision>\r\n      <geometry>\r\n        <cylinder radius="0.05" length="0.05"/>\r\n      </geometry>\r\n    </collision>\r\n    <inertial>\r\n      <mass value="0.2"/>\r\n      <inertia ixx="0.002" ixy="0" ixz="0" iyy="0.002" iyz="0" izz="0.003"/>\r\n    </inertial>\r\n  </link>\r\n\r\n  \x3c!-- LiDAR joint --\x3e\r\n  <joint name="lidar_joint" type="fixed">\r\n    <parent link="base_link"/>\r\n    <child link="lidar_link"/>\r\n    <origin xyz="0.1 0 0.2" rpy="0 0 0"/>\r\n  </joint>\r\n\r\n  \x3c!-- Gazebo plugins for Isaac ROS sensors --\x3e\r\n  <gazebo reference="camera_link">\r\n    <sensor type="camera" name="camera_sensor">\r\n      <update_rate>30</update_rate>\r\n      <camera name="head">\r\n        <horizontal_fov>1.3962634</horizontal_fov>\r\n        <image>\r\n          <width>1280</width>\r\n          <height>720</height>\r\n          <format>R8G8B8</format>\r\n        </image>\r\n        <clip>\r\n          <near>0.02</near>\r\n          <far>300</far>\r\n        </clip>\r\n      </camera>\r\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n        <frame_name>camera_link</frame_name>\r\n        <topic_name>camera/image_raw</topic_name>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\r\n\r\n  <gazebo reference="lidar_link">\r\n    <sensor type="ray" name="lidar_sensor">\r\n      <pose>0 0 0 0 0 0</pose>\r\n      <visualize>false</visualize>\r\n      <update_rate>10</update_rate>\r\n      <ray>\r\n        <scan>\r\n          <horizontal>\r\n            <samples>720</samples>\r\n            <resolution>1</resolution>\r\n            <min_angle>-3.14159</min_angle>\r\n            <max_angle>3.14159</max_angle>\r\n          </horizontal>\r\n        </scan>\r\n        <range>\r\n          <min>0.1</min>\r\n          <max>30.0</max>\r\n          <resolution>0.01</resolution>\r\n        </range>\r\n      </ray>\r\n      <plugin name="gpu_lidar" filename="libRaySensorGPU.so">\r\n        <ros>\r\n          <argument>~/scan:=scan</argument>\r\n        </ros>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\r\n</robot>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"implementing-visual-slam",children:"Implementing Visual SLAM"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"VSLAM Architecture",src:r(3411).A+"",width:"800",height:"400"})}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM is a technique that allows robots to map their environment and localize themselves within it using visual information. NVIDIA Isaac provides optimized VSLAM implementations."}),"\n",(0,s.jsx)(n.h3,{id:"isaac-sim-vslam-example",children:"Isaac Sim VSLAM Example"}),"\n",(0,s.jsx)(n.p,{children:"Let's implement a basic VSLAM pipeline:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import TransformStamped\r\nfrom tf2_ros import TransformBroadcaster\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nfrom stereo_msgs.msg import DisparityImage\r\nfrom nav_msgs.msg import Odometry\r\n\r\nclass VSLAMNode(Node):\r\n    """\r\n    A node that implements basic Visual SLAM functionality using Isaac ROS components\r\n    """\r\n    def __init__(self):\r\n        super().__init__(\'vslam_node\')\r\n        \r\n        # Initialize OpenCV bridge\r\n        self.bridge = CvBridge()\r\n        \r\n        # Subscribe to camera images\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        # Subscribe to camera info\r\n        self.info_subscription = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/camera_info\',\r\n            self.info_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher for odometry\r\n        self.odom_publisher = self.create_publisher(Odometry, \'/odom\', 10)\r\n        \r\n        # TF broadcaster\r\n        self.tf_broadcaster = TransformBroadcaster(self)\r\n        \r\n        # Internal variables\r\n        self.previous_image = None\r\n        self.current_position = np.array([0.0, 0.0, 0.0])\r\n        self.current_rotation = np.eye(3)\r\n        \r\n        # Feature detector and matcher (using CUDA if available)\r\n        try:\r\n            self.feature_detector = cv2.cuda.SURF_create(400)\r\n            self.use_gpu = True\r\n            self.get_logger().info("Using GPU for feature detection")\r\n        except:\r\n            self.feature_detector = cv2.SURF_create(400)\r\n            self.use_gpu = False\r\n            self.get_logger().info("Using CPU for feature detection")\r\n        \r\n        self.matcher = cv2.BFMatcher()\r\n        \r\n        self.get_logger().info("VSLAM node initialized")\r\n\r\n    def info_callback(self, msg):\r\n        """Handle camera intrinsic parameters"""\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.distortion_coeffs = np.array(msg.d)\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera frames for VSLAM"""\r\n        # Convert ROS image to OpenCV format\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\r\n        \r\n        if self.previous_image is None:\r\n            # Store first image for comparison\r\n            self.previous_image = cv_image\r\n            return\r\n        \r\n        # Detect and match features between current and previous frames\r\n        keypoints_prev, descriptors_prev = self.detect_features(self.previous_image)\r\n        keypoints_curr, descriptors_curr = self.detect_features(cv_image)\r\n        \r\n        if descriptors_prev is not None and descriptors_curr is not None:\r\n            # Match features\r\n            matches = self.matcher.match(descriptors_prev, descriptors_curr)\r\n            \r\n            # Sort matches by distance\r\n            matches = sorted(matches, key=lambda x: x.distance)\r\n            \r\n            # Extract matched keypoints\r\n            if len(matches) >= 10:  # Need minimum number of matches\r\n                src_pts = np.float32([keypoints_prev[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\r\n                dst_pts = np.float32([keypoints_curr[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\r\n                \r\n                # Estimate motion using Essential matrix\r\n                E, mask = cv2.findEssentialMat(src_pts, dst_pts, self.camera_matrix, \r\n                                              method=cv2.RANSAC, prob=0.999, threshold=1.0)\r\n                \r\n                if E is not None:\r\n                    # Recover pose\r\n                    _, R, t, _ = cv2.recoverPose(E, src_pts, dst_pts, self.camera_matrix)\r\n                    \r\n                    # Update position (scaled appropriately)\r\n                    scale = 0.1  # Placeholder scale factor - in real application this would come from depth estimation\r\n                    self.current_position += scale * self.current_rotation.dot(t.flatten())\r\n                    self.current_rotation = self.current_rotation.dot(R)\r\n                    \r\n                    # Publish odometry and TF\r\n                    self.publish_odometry(msg.header.stamp, msg.header.frame_id)\r\n        \r\n        # Update previous image\r\n        self.previous_image = cv_image\r\n\r\n    def detect_features(self, image):\r\n        """Detect features in an image using SURF"""\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n        \r\n        if self.use_gpu:\r\n            # Upload image to GPU\r\n            gpu_img = cv2.cuda_GpuMat()\r\n            gpu_img.upload(gray)\r\n            \r\n            # Detect keypoints and compute descriptors\r\n            keypoints_gpu, descriptors_gpu = self.feature_detector.detectAndCompute(gpu_img, None)\r\n            \r\n            # Download results\r\n            if descriptors_gpu is not None:\r\n                descriptors = descriptors_gpu.download()\r\n                keypoints = [cv2.KeyPoint(k.pt[0], k.pt[1], k.size, k.angle, k.response, k.octave, k.class_id) \r\n                            for k in keypoints_gpu]\r\n            else:\r\n                keypoints = []\r\n                descriptors = None\r\n        else:\r\n            # CPU-based feature detection\r\n            keypoints, descriptors = self.feature_detector.detectAndCompute(gray, None)\r\n        \r\n        return keypoints, descriptors\r\n\r\n    def publish_odometry(self, timestamp, frame_id):\r\n        """Publish odometry and transform"""\r\n        # Create odometry message\r\n        odom = Odometry()\r\n        odom.header.stamp = timestamp\r\n        odom.header.frame_id = frame_id\r\n        odom.child_frame_id = "base_footprint"\r\n        \r\n        # Set position\r\n        odom.pose.pose.position.x = self.current_position[0]\r\n        odom.pose.pose.position.y = self.current_position[1]\r\n        odom.pose.pose.position.z = self.current_position[2]\r\n        \r\n        # Set orientation from rotation matrix\r\n        # Convert rotation matrix to quaternion\r\n        qw = np.sqrt(max(0, 1 + self.current_rotation[0,0] + self.current_rotation[1,1] + self.current_rotation[2,2])) / 2\r\n        qx = np.sqrt(max(0, 1 + self.current_rotation[0,0] - self.current_rotation[1,1] - self.current_rotation[2,2])) / 2\r\n        qy = np.sqrt(max(0, 1 - self.current_rotation[0,0] + self.current_rotation[1,1] - self.current_rotation[2,2])) / 2\r\n        qz = np.sqrt(max(0, 1 - self.current_rotation[0,0] - self.current_rotation[1,1] + self.current_rotation[2,2])) / 2\r\n        q_sign = np.sign(self.current_rotation[2,1] - self.current_rotation[1,2])\r\n        qx = qx * np.where(q_sign > 0, 1, -1)\r\n        q_sign = np.sign(self.current_rotation[0,2] - self.current_rotation[2,0])\r\n        qy = qy * np.where(q_sign > 0, 1, -1)\r\n        q_sign = np.sign(self.current_rotation[1,0] - self.current_rotation[0,1])\r\n        qz = qz * np.where(q_sign > 0, 1, -1)\r\n        \r\n        odom.pose.pose.orientation.w = qw\r\n        odom.pose.pose.orientation.x = qx\r\n        odom.pose.pose.orientation.y = qy\r\n        odom.pose.pose.orientation.z = qz\r\n        \r\n        # Set velocities to zero for now (in a real implementation, estimate these)\r\n        odom.twist.twist.linear.x = 0.0\r\n        odom.twist.twist.linear.y = 0.0\r\n        odom.twist.twist.linear.z = 0.0\r\n        odom.twist.twist.angular.x = 0.0\r\n        odom.twist.twist.angular.y = 0.0\r\n        odom.twist.twist.angular.z = 0.0\r\n        \r\n        # Publish odometry\r\n        self.odom_publisher.publish(odom)\r\n        \r\n        # Broadcast transform\r\n        t = TransformStamped()\r\n        t.header.stamp = timestamp\r\n        t.header.frame_id = \'odom\'\r\n        t.child_frame_id = \'base_footprint\'\r\n        t.transform.translation.x = self.current_position[0]\r\n        t.transform.translation.y = self.current_position[1]\r\n        t.transform.translation.z = self.current_position[2]\r\n        t.transform.rotation.w = qw\r\n        t.transform.rotation.x = qx\r\n        t.transform.rotation.y = qy  \r\n        t.transform.rotation.z = qz\r\n        \r\n        self.tf_broadcaster.sendTransform(t)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    \r\n    vslam_node = VSLAMNode()\r\n    \r\n    try:\r\n        rclpy.spin(vslam_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    \r\n    vslam_node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-fusion-with-isaac",children:"Sensor Fusion with Isaac"}),"\n",(0,s.jsx)(n.p,{children:"NVIDIA Isaac provides powerful sensor fusion capabilities that combine data from multiple sensors for more robust perception:"}),"\n",(0,s.jsx)(n.h3,{id:"multi-sensor-data-integration",children:"Multi-sensor Data Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, LaserScan, PointCloud2\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom std_msgs.msg import Float32\r\nimport numpy as np\r\nimport sensor_msgs.point_cloud2 as pc2\r\n\r\nclass SensorFusionNode(Node):\r\n    """\r\n    A node that implements sensor fusion combining camera, LiDAR, and other sensors\r\n    """\r\n    def __init__(self):\r\n        super().__init__(\'sensor_fusion_node\')\r\n        \r\n        # Subscribers for different sensors\r\n        self.camera_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.camera_callback,\r\n            10\r\n        )\r\n        \r\n        self.lidar_subscription = self.create_subscription(\r\n            LaserScan,\r\n            \'/scan\',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n        \r\n        self.imu_subscription = self.create_subscription(\r\n            # IMU message type would go here\r\n            # For brevity, we\'ll assume a generic sensor message\r\n            Float32,  # Placeholder - in a real implementation this would be sensor_msgs/Imu\r\n            \'/imu/data\',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher for fused data\r\n        self.fused_publisher = self.create_publisher(PoseStamped, \'/fused_pose\', 10)\r\n        \r\n        # Internal state\r\n        self.camera_data = None\r\n        self.lidar_data = None\r\n        self.imu_data = None\r\n        \r\n        self.get_logger().info("Sensor fusion node initialized")\r\n\r\n    def camera_callback(self, msg):\r\n        """Process camera data"""\r\n        self.camera_data = msg\r\n        self.perform_fusion()\r\n\r\n    def lidar_callback(self, msg):\r\n        """Process LiDAR data"""\r\n        self.lidar_data = msg\r\n        self.perform_fusion()\r\n\r\n    def imu_callback(self, msg):\r\n        """Process IMU data"""\r\n        self.imu_data = msg\r\n        self.perform_fusion()\r\n\r\n    def perform_fusion(self):\r\n        """Perform sensor fusion to estimate pose"""\r\n        # This is a simplified example - real fusion would use more sophisticated algorithms\r\n        # like Extended Kalman Filter or Particle Filter\r\n        \r\n        if self.camera_data is not None and self.lidar_data is not None:\r\n            # Combine visual and LiDAR estimates\r\n            # In a real implementation, this would involve:\r\n            # 1. Visual-inertial odometry from camera and IMU\r\n            # 2. LiDAR-inertial odometry from laser scanner and IMU\r\n            # 3. Fusion of all estimates using an extended Kalman filter\r\n            \r\n            # Placeholder: publish a combined estimate\r\n            fused_pose = PoseStamped()\r\n            fused_pose.header.stamp = self.get_clock().now().to_msg()\r\n            fused_pose.header.frame_id = "odom"\r\n            # Set pose based on fusion results\r\n            fused_pose.pose.position.x = 0.0  # Placeholder\r\n            fused_pose.pose.position.y = 0.0  # Placeholder\r\n            fused_pose.pose.position.z = 0.0  # Placeholder\r\n            \r\n            self.fused_publisher.publish(fused_pose)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    \r\n    fusion_node = SensorFusionNode()\r\n    \r\n    try:\r\n        rclpy.spin(fusion_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    \r\n    fusion_node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-perception-pipeline",children:"Isaac Perception Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"The Isaac perception pipeline combines multiple AI models and perception techniques:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Detection"}),": Identify and locate objects in the scene"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Semantic Segmentation"}),": Classify each pixel in the image"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Instance Segmentation"}),": Differentiate between individual instances of objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth Estimation"}),": Generate depth information from monocular images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose Estimation"}),": Determine 6DOF pose of objects"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-application",children:"Practical Application"}),"\n",(0,s.jsx)(n.p,{children:"In robotics, perception systems using NVIDIA Isaac are used for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Autonomous Navigation"}),": Identifying pathways and obstacles"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Manipulation"}),": Recognizing and grasping objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human-Robot Interaction"}),": Understanding gestures and expressions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quality Inspection"}),": Detecting defects in manufacturing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Search and Rescue"}),": Identifying people and hazards"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-exercise-preview",children:"Lab Exercise Preview"}),"\n",(0,s.jsx)(n.p,{children:"In the next section, you'll find the detailed instructions for the NVIDIA Isaac lab exercise, where you'll implement a complete perception pipeline with object detection and tracking."}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In these weeks, you've learned:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How to set up NVIDIA Isaac for perception tasks"}),"\n",(0,s.jsx)(n.li,{children:"How to implement VSLAM systems for robot navigation"}),"\n",(0,s.jsx)(n.li,{children:"How to perform sensor fusion with multiple modalities"}),"\n",(0,s.jsx)(n.li,{children:"How to process sensor data with CUDA acceleration"}),"\n",(0,s.jsx)(n.li,{children:"How to integrate perception systems with other ROS 2 nodes"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"navigation",children:"Navigation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"/nvidia-isaac/intro",children:"\u2190 Previous: Introduction to NVIDIA Isaac"})," | ",(0,s.jsx)(n.a,{href:"/nvidia-isaac/week9",children:"Next: Week 9: AI-robot Brain Integration"})," | ",(0,s.jsx)(n.a,{href:"/nvidia-isaac/intro",children:"Module Home"})]}),"\n",(0,s.jsxs)(n.p,{children:["Continue to ",(0,s.jsx)(n.a,{href:"/nvidia-isaac/week9",children:"Week 9: AI-robot Brain Integration"})," to explore reinforcement learning and advanced AI integration."]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var i=r(6540);const s={},a=i.createContext(s);function t(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(a.Provider,{value:n},e.children)}},9161:(e,n,r)=>{r.d(n,{A:()=>i});const i="data:image/png;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNjAwIj48cmVjdCB3aWR0aD0iODAwIiBoZWlnaHQ9IjYwMCIgZmlsbD0iI2U4ZjRmZCIvPjx0ZXh0IHg9IjQwMCIgeT0iMzAwIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iMjQiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiMxYTczZTgiPk5WSURJQSBJc2FhYyBQZXJjZXB0aW9uIFBpcGVsaW5lPC90ZXh0Pjx0ZXh0IHg9IjQwMCIgeT0iMzUwIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iMTYiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiM1ZjYzNjgiPkFJLWRyaXZlbiBwZXJjZXB0aW9uIGZvciByb2JvdGljcyB1c2luZyBJc2FhYyBwbGF0Zm9ybTwvdGV4dD48L3N2Zz4="}}]);