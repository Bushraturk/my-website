"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1435],{5362:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"vla/lab-exercises/lab1","title":"Lab Exercise - Vision-Language-Action Integration","description":"Objective","source":"@site/docs/vla/lab-exercises/lab1.md","sourceDirName":"vla/lab-exercises","slug":"/vla/lab-exercises/lab1","permalink":"/my-website/vla/lab-exercises/lab1","draft":false,"unlisted":false,"editUrl":"https://github.com/Bushraturk/my-website/edit/main/docs/docs/vla/lab-exercises/lab1.md","tags":[],"version":"current","sidebarPosition":21,"frontMatter":{"title":"Lab Exercise - Vision-Language-Action Integration","sidebar_position":21},"sidebar":"textbookSidebar","previous":{"title":"Week 13 - Course Synthesis and Capstone Project","permalink":"/my-website/vla/week13"},"next":{"title":"Quiz 1 - Vision-Language-Action Fundamentals","permalink":"/my-website/vla/assessments/quiz1"}}');var a=r(4848),i=r(8453);const o={title:"Lab Exercise - Vision-Language-Action Integration",sidebar_position:21},s="Lab Exercise: Vision-Language-Action Integration",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Equipment Required",id:"equipment-required",level:2},{value:"Lab Steps",id:"lab-steps",level:2},{value:"Step 1: Environment Setup and Verification",id:"step-1-environment-setup-and-verification",level:3},{value:"Step 2: Implement the VLA System Architecture",id:"step-2-implement-the-vla-system-architecture",level:3},{value:"Step 3: Create a Launch File",id:"step-3-create-a-launch-file",level:3},{value:"Step 4: Testing Your VLA System",id:"step-4-testing-your-vla-system",level:3},{value:"Step 5: Evaluating Performance",id:"step-5-evaluating-performance",level:3},{value:"Lab Report",id:"lab-report",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Extension Activities",id:"extension-activities",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lab-exercise-vision-language-action-integration",children:"Lab Exercise: Vision-Language-Action Integration"})}),"\n",(0,a.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,a.jsx)(n.p,{children:"In this lab exercise, you will implement a complete Vision-Language-Action (VLA) system that can interpret natural language commands, perceive the environment, and execute appropriate robotic actions. You'll build a system that combines all the concepts learned throughout the course into a functioning embodied AI agent."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"After completing this lab, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Integrate vision, language, and action components into a unified system"}),"\n",(0,a.jsx)(n.li,{children:"Process natural language commands to generate executable robot actions"}),"\n",(0,a.jsx)(n.li,{children:"Implement grounding mechanisms that connect language to perception"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the performance of a VLA system in simulated and/or physical environments"}),"\n",(0,a.jsx)(n.li,{children:"Troubleshoot common integration challenges in multi-modal systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Completion of all previous modules (ROS 2, Gazebo/Unity, NVIDIA Isaac, VLA)"}),"\n",(0,a.jsx)(n.li,{children:"Access to a robot platform (simulated or physical) with camera and basic manipulation capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Understanding of VLA model architectures and training approaches"}),"\n",(0,a.jsx)(n.li,{children:"Experience with ROS 2 messaging and action servers"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"equipment-required",children:"Equipment Required"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Robot platform with camera and manipulation capability (simulated or physical)"}),"\n",(0,a.jsx)(n.li,{children:"Computer with NVIDIA GPU and CUDA support"}),"\n",(0,a.jsx)(n.li,{children:"Installed ROS 2, Isaac ROS packages, and VLA model dependencies"}),"\n",(0,a.jsx)(n.li,{children:"Gazebo simulation environment (if using simulation)"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lab-steps",children:"Lab Steps"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-environment-setup-and-verification",children:"Step 1: Environment Setup and Verification"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Verify all required software components are installed:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Check for ROS 2 installation\r\nprintenv | grep ROS\r\n\r\n# Verify Isaac ROS packages\r\nros2 pkg list | grep isaac\r\n\r\n# Check GPU availability\r\nnvidia-smi\r\n\r\n# Verify VLA model dependencies\r\npython -c \"import transformers; import torch; print('Dependencies OK')\"\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Set up the workspace for the integration project:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/vla_integration_ws/src\r\ncd ~/vla_integration_ws/src\r\ngit clone https://github.com/nvidia/cuvl_benchmark.git  # Example VLA model\r\ncd ..\r\ncolcon build --symlink-install\r\nsource install/setup.bash\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"step-2-implement-the-vla-system-architecture",children:"Step 2: Implement the VLA System Architecture"}),"\n",(0,a.jsx)(n.p,{children:"Create the main integration node that connects language understanding to action execution:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\r\nfrom visualization_msgs.msg import Marker, MarkerArray\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport torch\r\nfrom transformers import CLIPProcessor, CLIPModel\r\nimport json\r\nfrom typing import List, Dict, Any, Optional\r\nimport tf_transformations\r\n\r\n\r\nclass VLASystemNode(Node):\r\n    """\r\n    Vision-Language-Action system that interprets natural language commands,\r\n    perceives the environment, and executes appropriate robotic actions.\r\n    """\r\n    def __init__(self):\r\n        super().__init__(\'vla_system_node\')\r\n        \r\n        # Initialize CV bridge for image processing\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Initialize perception components\r\n        self.setup_perception()\r\n        \r\n        # Initialize language processing components\r\n        self.setup_language_processing()\r\n        \r\n        # Initialize action execution components\r\n        self.setup_action_execution()\r\n        \r\n        # Subscribe to necessary topics\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        self.command_subscription = self.create_subscription(\r\n            String,\r\n            \'/natural_language_command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher for system status\r\n        self.status_publisher = self.create_publisher(String, \'/vla_system/status\', 10)\r\n        \r\n        # Publisher for markers for visualization\r\n        self.marker_publisher = self.create_publisher(MarkerArray, \'/vla_system/markers\', 10)\r\n        \r\n        # Store latest perception data\r\n        self.latest_image = None\r\n        self.detected_objects = []\r\n        \r\n        self.get_logger().info("VLA System initialized successfully")\r\n\r\n    def setup_perception(self):\r\n        """Initialize perception components using Isaac ROS packages"""\r\n        self.get_logger().info("Setting up perception components...")\r\n        \r\n        # Initialize object detection model (using Isaac ROS approach)\r\n        # For this example, we\'ll use a placeholder - in practice, use Isaac ROS Detection2D packages\r\n        try:\r\n            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\r\n            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\r\n            self.get_logger().info("CLIP model loaded for vision-language understanding")\r\n        except Exception as e:\r\n            self.get_logger().warn(f"Could not load CLIP model: {e}. Using placeholder.")\r\n\r\n    def setup_language_processing(self):\r\n        """Initialize language processing components"""\r\n        self.get_logger().info("Setting up language processing components...")\r\n        \r\n        # In a full implementation, this would connect to a VLA model\r\n        # For this lab, we\'ll use a simplified approach\r\n        self.object_keywords = {\r\n            "water bottle": ["water", "bottle", "drink"],\r\n            "cup": ["cup", "mug", "glass"],\r\n            "book": ["book", "novel", "textbook"],\r\n            "keyboard": ["keyboard", "typing", "computer"],\r\n            "mouse": ["mouse", "computer_mouse"]\r\n        }\r\n        \r\n        # Spatial relation keywords\r\n        self.spatial_keywords = {\r\n            "left": ["left", "left side", "to the left"],\r\n            "right": ["right", "right side", "to the right"],\r\n            "front": ["front", "in front", "ahead"],\r\n            "behind": ["behind", "back", "rear"],\r\n            "near": ["near", "close", "by", "next to"],\r\n            "on": ["on", "above", "atop"],\r\n            "under": ["under", "below", "beneath"]\r\n        }\r\n\r\n    def setup_action_execution(self):\r\n        """Initialize action execution components"""\r\n        self.get_logger().info("Setting up action execution components...")\r\n        \r\n        # Navigation client\r\n        self.nav_client = self.create_client(NavigateToPose, \'/navigate_to_pose\')\r\n        \r\n        # Manipulation client (using MoveIt or similar)\r\n        # In practice, this would connect to your robot\'s manipulation interface\r\n\r\n        # Wait for navigation service availability\r\n        while not self.nav_client.wait_for_service(timeout_sec=1.0):\r\n            self.get_logger().info(\'Navigation service not available, waiting again...\')\r\n\r\n    def image_callback(self, msg: Image):\r\n        """Process incoming camera images for perception"""\r\n        try:\r\n            # Convert ROS image to OpenCV format\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \'bgr8\')\r\n            self.latest_image = cv_image\r\n            \r\n            # Perform object detection (simplified for this example)\r\n            self.detected_objects = self.simple_object_detection(cv_image)\r\n            \r\n            # In a real implementation, this would use Isaac ROS perception packages\r\n            # such as Isaac ROS Detection2D, Isaac ROS Visual Slam, etc.\r\n            \r\n            self.get_logger().debug(f"Detected {len(self.detected_objects)} objects")\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in image_callback: {e}")\r\n\r\n    def command_callback(self, msg: String):\r\n        """Process natural language command and execute appropriate action"""\r\n        command = msg.data\r\n        self.get_logger().info(f"Received command: \'{command}\'")\r\n        \r\n        try:\r\n            # Publish status update\r\n            status_msg = String()\r\n            status_msg.data = f"Processing command: {command}"\r\n            self.status_publisher.publish(status_msg)\r\n            \r\n            # Parse the command using vision-language understanding\r\n            parsed_command = self.parse_language_command(command, self.detected_objects)\r\n            \r\n            if parsed_command:\r\n                # Execute the parsed action plan\r\n                success = self.execute_action_plan(parsed_command)\r\n                \r\n                # Report results\r\n                result_msg = String()\r\n                if success:\r\n                    result_msg.data = f"Successfully executed: {command}"\r\n                    self.get_logger().info(f"Command executed successfully: {command}")\r\n                else:\r\n                    result_msg.data = f"Failed to execute: {command}"\r\n                    self.get_logger().error(f"Failed to execute command: {command}")\r\n                    \r\n                self.status_publisher.publish(result_msg)\r\n            else:\r\n                error_msg = String()\r\n                error_msg.data = f"Could not parse command: {command}"\r\n                self.status_publisher.publish(error_msg)\r\n                \r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing command \'{command}\': {e}")\r\n            error_msg = String()\r\n            error_msg.data = f"Error processing command: {str(e)}"\r\n            self.status_publisher.publish(error_msg)\r\n\r\n    def parse_language_command(self, command: str, detected_objects: List[Dict]) -> Optional[Dict[str, Any]]:\r\n        """\r\n        Parse a natural language command using vision-language understanding.\r\n        \r\n        Args:\r\n            command: Natural language command\r\n            detected_objects: List of objects detected in the current scene\r\n            \r\n        Returns:\r\n            Dictionary containing the parsed action plan, or None if parsing failed\r\n        """\r\n        command_lower = command.lower()\r\n        \r\n        # Identify action verb\r\n        action = None\r\n        if any(word in command_lower for word in ["go to", "navigate to", "move to", "approach"]):\r\n            action = "navigate"\r\n        elif any(word in command_lower for word in ["pick up", "grasp", "take", "grab"]):\r\n            action = "pick"\r\n        elif any(word in command_lower for word in ["put", "place", "drop", "set"]):\r\n            action = "place"\r\n        elif any(word in command_lower for word in ["bring", "fetch", "deliver"]):\r\n            action = "fetch"\r\n        else:\r\n            self.get_logger().warn(f"Could not identify action in command: {command}")\r\n            return None\r\n        \r\n        # Extract object reference\r\n        target_object = None\r\n        for obj_name, keywords in self.object_keywords.items():\r\n            if any(keyword in command_lower for keyword in keywords):\r\n                # Find the object in the detected objects\r\n                for detected_obj in detected_objects:\r\n                    if detected_obj[\'name\'] == obj_name:\r\n                        target_object = detected_obj\r\n                        break\r\n                if target_object:\r\n                    break\r\n        \r\n        # Extract spatial reference\r\n        target_location = None\r\n        for location_keyword, synonyms in self.spatial_keywords.items():\r\n            if any(synonym in command_lower for synonym in synonyms):\r\n                target_location = location_keyword\r\n                break\r\n        \r\n        # Create action plan\r\n        action_plan = {\r\n            "action": action,\r\n            "target_object": target_object,\r\n            "target_location": target_location,\r\n            "original_command": command,\r\n            "confidence": 0.8  # Placeholder confidence\r\n        }\r\n        \r\n        self.get_logger().info(f"Parsed command into action plan: {action_plan}")\r\n        return action_plan\r\n\r\n    def simple_object_detection(self, image):\r\n        """\r\n        Simple object detection for demonstration purposes.\r\n        In a real implementation, this would use Isaac ROS detection packages.\r\n        """\r\n        # This is a placeholder - in practice, use Isaac ROS Vision packages\r\n        # like Isaac ROS Detection2D to perform object detection\r\n        \r\n        # For demonstration, return some predefined objects with positions\r\n        return [\r\n            {\r\n                "name": "water bottle",\r\n                "bbox": [100, 100, 200, 200],  # [x, y, width, height]\r\n                "position_3d": [1.0, 0.5, 0.0],  # x, y, z in robot frame\r\n                "confidence": 0.9\r\n            },\r\n            {\r\n                "name": "cup",\r\n                "bbox": [300, 150, 150, 150],\r\n                "position_3d": [1.2, -0.2, 0.0],\r\n                "confidence": 0.85\r\n            }\r\n        ]\r\n\r\n    def execute_action_plan(self, action_plan: Dict[str, Any]) -> bool:\r\n        """\r\n        Execute a parsed action plan.\r\n        \r\n        Args:\r\n            action_plan: Dictionary containing the action to execute\r\n            \r\n        Returns:\r\n            True if execution was successful, False otherwise\r\n        """\r\n        action = action_plan.get("action")\r\n        \r\n        if action == "navigate":\r\n            return self.execute_navigation_action(action_plan)\r\n        elif action == "pick":\r\n            return self.execute_pick_action(action_plan)\r\n        elif action == "place":\r\n            return self.execute_place_action(action_plan)\r\n        elif action == "fetch":\r\n            return self.execute_fetch_action(action_plan)\r\n        else:\r\n            self.get_logger().error(f"Unknown action: {action}")\r\n            return False\r\n\r\n    def execute_navigation_action(self, action_plan: Dict[str, Any]) -> bool:\r\n        """Execute navigation action."""\r\n        target_object = action_plan.get("target_object")\r\n        \r\n        if target_object and "position_3d" in target_object:\r\n            target_pos = target_object["position_3d"]\r\n            self.get_logger().info(f"Navigating to object at position: {target_pos}")\r\n            \r\n            # Create navigation goal\r\n            goal_msg = NavigateToPose.Goal()\r\n            goal_msg.pose.header.frame_id = "map"\r\n            goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\r\n            goal_msg.pose.pose.position.x = target_pos[0]\r\n            goal_msg.pose.pose.position.y = target_pos[1]\r\n            goal_msg.pose.pose.position.z = 0.0\r\n            \r\n            # Set orientation (face towards object)\r\n            quat = tf_transformations.quaternion_from_euler(0, 0, 0)\r\n            goal_msg.pose.pose.orientation.x = quat[0]\r\n            goal_msg.pose.pose.orientation.y = quat[1]\r\n            goal_msg.pose.pose.orientation.z = quat[2]\r\n            goal_msg.pose.pose.orientation.w = quat[3]\r\n            \r\n            # Send navigation goal\r\n            future = self.nav_client.call_async(goal_msg)\r\n            rclpy.spin_until_future_complete(self, future)\r\n            \r\n            if future.result() is not None:\r\n                self.get_logger().info("Navigation goal sent successfully")\r\n                return True\r\n            else:\r\n                self.get_logger().error("Navigation goal failed")\r\n                return False\r\n        else:\r\n            self.get_logger().warn("No target object or position found for navigation")\r\n            return False\r\n\r\n    def execute_pick_action(self, action_plan: Dict[str, Any]) -> bool:\r\n        """Execute pick action."""\r\n        target_object = action_plan.get("target_object")\r\n        \r\n        if target_object:\r\n            self.get_logger().info(f"Attempting to pick up: {target_object[\'name\']}")\r\n            \r\n            # In a real implementation, this would send a manipulation command\r\n            # such as a MoveIt pick action to the robot\'s manipulation interface\r\n            # For simulation purposes, we\'ll just return success\r\n            \r\n            # Publish visualization marker for picked object\r\n            self.publish_object_marker(target_object, "picked")\r\n            \r\n            self.get_logger().info(f"Successfully picked up: {target_object[\'name\']}")\r\n            return True\r\n        else:\r\n            self.get_logger().warn("No target object specified for pick action")\r\n            return False\r\n\r\n    def execute_place_action(self, action_plan: Dict[str, Any]) -> bool:\r\n        """Execute place action."""\r\n        target_location = action_plan.get("target_location")\r\n        \r\n        if target_location:\r\n            self.get_logger().info(f"Attempting to place object at: {target_location}")\r\n            \r\n            # In a real implementation, this would send a manipulation command\r\n            # such as a MoveIt place action to the robot\'s manipulation interface\r\n            # For simulation purposes, we\'ll just return success\r\n            \r\n            self.get_logger().info(f"Successfully placed object at: {target_location}")\r\n            return True\r\n        else:\r\n            self.get_logger().warn("No target location specified for place action")\r\n            return False\r\n\r\n    def execute_fetch_action(self, action_plan: Dict[str, Any]) -> bool:\r\n        """Execute fetch action (pick and place)."""\r\n        # For fetch, we need to first navigate to the object, pick it up, \r\n        # then navigate to the destination and place it\r\n        success = True\r\n        \r\n        # Navigate to target object\r\n        nav_success = self.execute_navigation_action(action_plan)\r\n        if not nav_success:\r\n            self.get_logger().error("Failed to navigate to target object")\r\n            return False\r\n            \r\n        # Pick up the object\r\n        pick_success = self.execute_pick_action(action_plan)\r\n        if not pick_success:\r\n            self.get_logger().error("Failed to pick up target object")\r\n            return False\r\n            \r\n        # In a real implementation, we would then navigate to the delivery location\r\n        # and execute the place action. For simplicity in this lab, we\'ll return success.\r\n        \r\n        self.get_logger().info("Fetch action completed successfully")\r\n        return True\r\n\r\n    def publish_object_marker(self, obj: Dict[str, Any], status: str):\r\n        """Publish a visualization marker for the object."""\r\n        try:\r\n            marker_array = MarkerArray()\r\n            \r\n            marker = Marker()\r\n            marker.header.frame_id = "map"  # Could be changed based on robot\'s frame\r\n            marker.header.stamp = self.get_clock().now().to_msg()\r\n            marker.ns = "vla_objects"\r\n            marker.id = hash(obj["name"]) % 1000  # Simple ID generation\r\n            marker.type = Marker.CUBE\r\n            marker.action = Marker.ADD\r\n            \r\n            # Set position\r\n            pos = obj.get("position_3d", [0, 0, 0])\r\n            marker.pose.position.x = pos[0]\r\n            marker.pose.position.y = pos[1]\r\n            marker.pose.position.z = pos[2] + 0.1  # Offset to make visible above ground\r\n            \r\n            # Set orientation\r\n            marker.pose.orientation.w = 1.0\r\n            \r\n            # Set dimensions\r\n            marker.scale.x = 0.1\r\n            marker.scale.y = 0.1\r\n            marker.scale.z = 0.1\r\n            \r\n            # Set color based on status\r\n            if status == "picked":\r\n                marker.color.r = 1.0\r\n                marker.color.g = 0.0\r\n                marker.color.b = 0.0\r\n            elif status == "target":\r\n                marker.color.r = 0.0\r\n                marker.color.g = 1.0\r\n                marker.color.b = 0.0\r\n            else:\r\n                marker.color.r = 1.0\r\n                marker.color.g = 1.0\r\n                marker.color.b = 0.0\r\n                \r\n            marker.color.a = 1.0\r\n            \r\n            # Set text\r\n            marker.text = f"{obj[\'name\']} ({status})"\r\n            \r\n            marker_array.markers.append(marker)\r\n            self.marker_publisher.publish(marker_array)\r\n        except Exception as e:\r\n            self.get_logger().warn(f"Could not publish object marker: {e}")\r\n\r\n\r\ndef main(args=None):\r\n    """Main function to run the VLA system node."""\r\n    rclpy.init(args=args)\r\n    \r\n    vla_system = VLASystemNode()\r\n    \r\n    try:\r\n        rclpy.spin(vla_system)\r\n    except KeyboardInterrupt:\r\n        vla_system.get_logger().info("Shutting down VLA system node...")\r\n    finally:\r\n        vla_system.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-create-a-launch-file",children:"Step 3: Create a Launch File"}),"\n",(0,a.jsx)(n.p,{children:"Create a launch file to start the complete VLA system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- vla_integration_pkg/launch/vla_system.launch.py --\x3e\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom ament_index_python.packages import get_package_share_directory\r\nimport os\r\n\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    namespace = LaunchConfiguration('namespace')\r\n    use_sim_time = LaunchConfiguration('use_sim_time')\r\n    \r\n    namespace_arg = DeclareLaunchArgument(\r\n        'namespace',\r\n        default_value='',\r\n        description='Namespace for the VLA system nodes'\r\n    )\r\n    \r\n    use_sim_time_arg = DeclareLaunchArgument(\r\n        'use_sim_time', \r\n        default_value='false',\r\n        description='Use simulation time if true'\r\n    )\r\n    \r\n    # VLA system node\r\n    vla_system_node = Node(\r\n        package='vla_integration_pkg',\r\n        executable='vla_system_node',\r\n        name='vla_system',\r\n        namespace=namespace,\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time}\r\n        ],\r\n        remappings=[\r\n            ('/camera/image_raw', '/camera/color/image_raw'),  # Remap to actual camera topic\r\n            ('/natural_language_command', '/user_commands'),\r\n            ('/vla_system/status', '/system_status')\r\n        ],\r\n        output='screen'\r\n    )\r\n    \r\n    # Perception pipeline (using Isaac ROS components)\r\n    perception_node = Node(\r\n        package='isaac_ros_detectnet',\r\n        executable='isaac_ros_detectnet',\r\n        name='perception_pipeline',\r\n        namespace=namespace,\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time},\r\n            {'model_name': 'ssd_mobilenet_v2_coco'},\r\n            {'input/image_width': 640},\r\n            {'input/image_height': 480}\r\n        ]\r\n    )\r\n    \r\n    # Navigation system\r\n    navigation_node = Node(\r\n        package='nav2_bringup',\r\n        executable='navigation2',\r\n        name='navigation_system',\r\n        namespace=namespace,\r\n        parameters=[\r\n            {'use_sim_time': use_sim_time}\r\n        ]\r\n    )\r\n    \r\n    return LaunchDescription([\r\n        namespace_arg,\r\n        use_sim_time_arg,\r\n        vla_system_node,\r\n        perception_node,\r\n        navigation_node\r\n    ])\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-4-testing-your-vla-system",children:"Step 4: Testing Your VLA System"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"In a terminal, start your robot simulation (or bring up your physical robot):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# If using simulation\r\nros2 launch your_robot_gazebo your_robot_world.launch.py\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"In another terminal, start your VLA system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/vla_integration_ws\r\nsource install/setup.bash\r\nros2 run vla_integration_pkg vla_system_node\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"In a third terminal, send test commands to your system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Command the robot to navigate to a detected object\r\nros2 topic pub /natural_language_command std_msgs/String \"data: 'Go to the water bottle'\"\r\n\r\n# Command the robot to pick up an object\r\nros2 topic pub /natural_language_command std_msgs/String \"data: 'Pick up the cup'\"\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Monitor the system's behavior:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Check the status of your VLA system\r\nros2 topic echo /vla_system/status\r\n\r\n# View detected objects and their positions\r\nros2 run rviz2 rviz2\r\n# Then add the marker topic to visualize object positions\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"step-5-evaluating-performance",children:"Step 5: Evaluating Performance"}),"\n",(0,a.jsx)(n.p,{children:"Evaluate your VLA system using the following metrics:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Command Success Rate"}),": Percentage of commands successfully executed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception Accuracy"}),": How accurately the system identifies objects and their positions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Response Time"}),": How quickly the system processes and responds to commands"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Robustness"}),": How well the system handles ambiguous commands or challenging lighting conditions"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lab-report",children:"Lab Report"}),"\n",(0,a.jsx)(n.p,{children:"Submit a lab report including:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"System Design"}),": Describe your VLA system architecture and design decisions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implementation Details"}),": Explain key components and their interactions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Results"}),": Document the performance of your system for various commands"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Challenges"}),": Describe any obstacles encountered and how you addressed them"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Improvements"}),": Propose ways to enhance your VLA system"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"GPU Memory Issues"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Problem: System runs out of GPU memory when running VLA models"}),"\n",(0,a.jsx)(n.li,{children:"Solution: Reduce batch size, use model quantization, or use CPU for some components"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Perception Failures"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Problem: Objects not being detected or localized accurately"}),"\n",(0,a.jsx)(n.li,{children:"Solution: Improve lighting, adjust detection thresholds, or calibrate camera"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Language Understanding Errors"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Problem: Commands not being parsed correctly"}),"\n",(0,a.jsx)(n.li,{children:"Solution: Expand keyword database or implement more sophisticated NLP processing"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Action Execution Failures"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Problem: Robot not executing planned actions correctly"}),"\n",(0,a.jsx)(n.li,{children:"Solution: Check robot calibration, verify action server availability, adjust planning parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"extension-activities",children:"Extension Activities"}),"\n",(0,a.jsx)(n.p,{children:"For advanced learners, consider implementing:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-Modal Grounding"}),": Connect language to specific visual features"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Uncertainty Handling"}),": Implement confidence-aware execution with fallback behaviors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Learning from Corrections"}),": Allow the system to improve based on user feedback"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Collaborative Behaviors"}),": Implement teamwork with multiple robots or humans"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"In this lab, you've implemented a complete Vision-Language-Action system that demonstrates the integration of perception, cognition, and action. You've created an embodied AI system capable of understanding natural language commands and executing appropriate robotic behaviors grounded in visual perception."}),"\n",(0,a.jsx)(n.p,{children:"This concludes the hands-on portion of the VLA module and the entire Physical AI & Humanoid Robotics course. You now have the knowledge and skills to build sophisticated embodied AI systems that connect language to action through perception."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var t=r(6540);const a={},i=t.createContext(a);function o(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);