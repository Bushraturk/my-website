"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8259],{7679:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>g,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"vla/week10-11","title":"Week 10-11 - Vision-Language Integration and Foundation Models","description":"In these two weeks, we\'ll explore how modern AI models integrate visual perception with language understanding to enable robots to interpret natural language commands and connect them to real-world visual observations. This represents a fundamental advance in embodied intelligence where language and vision are grounded in physical reality.","source":"@site/docs/vla/week10-11.md","sourceDirName":"vla","slug":"/vla/week10-11","permalink":"/my-website/vla/week10-11","draft":false,"unlisted":false,"editUrl":"https://github.com/Bushraturk/my-website/edit/main/docs/docs/vla/week10-11.md","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"title":"Week 10-11 - Vision-Language Integration and Foundation Models","sidebar_position":17},"sidebar":"textbookSidebar","previous":{"title":"Introduction to Vision-Language-Action (VLA) Models","permalink":"/my-website/vla/intro"},"next":{"title":"Week 12 - Action Planning with Large Language Models","permalink":"/my-website/vla/week12"}}');var t=i(4848),a=i(8453);const o={title:"Week 10-11 - Vision-Language Integration and Foundation Models",sidebar_position:17},s="Week 10-11: Vision-Language Integration and Foundation Models",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Vision-Language Models",id:"introduction-to-vision-language-models",level:2},{value:"Key Challenges in Vision-Language Integration",id:"key-challenges-in-vision-language-integration",level:3},{value:"Foundation Models for Robotics",id:"foundation-models-for-robotics",level:2},{value:"Contrastive Language-Image Pre-training (CLIP)",id:"contrastive-language-image-pre-training-clip",level:3},{value:"Robotics-Specific Adaptations",id:"robotics-specific-adaptations",level:3},{value:"Implementing Vision-Language Integration",id:"implementing-vision-language-integration",level:2},{value:"Multimodal Fusion Techniques",id:"multimodal-fusion-techniques",level:2},{value:"Early Fusion",id:"early-fusion",level:3},{value:"Late Fusion",id:"late-fusion",level:3},{value:"Cross-Attention Fusion",id:"cross-attention-fusion",level:3},{value:"Vision-Language Datasets for Robotics",id:"vision-language-datasets-for-robotics",level:2},{value:"Transfer Learning with Pre-trained Models",id:"transfer-learning-with-pre-trained-models",level:2},{value:"Practical Applications",id:"practical-applications",level:2},{value:"Lab Exercise Preview",id:"lab-exercise-preview",level:2},{value:"Summary",id:"summary",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"week-10-11-vision-language-integration-and-foundation-models",children:"Week 10-11: Vision-Language Integration and Foundation Models"})}),"\n",(0,t.jsx)(e.p,{children:"In these two weeks, we'll explore how modern AI models integrate visual perception with language understanding to enable robots to interpret natural language commands and connect them to real-world visual observations. This represents a fundamental advance in embodied intelligence where language and vision are grounded in physical reality."}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the architecture of multimodal models that connect vision and language"}),"\n",(0,t.jsx)(e.li,{children:"Implement vision-language models for robotic task understanding"}),"\n",(0,t.jsx)(e.li,{children:"Fine-tune pre-trained models on robotic datasets"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate the performance of vision-language models in robotic contexts"}),"\n",(0,t.jsx)(e.li,{children:"Connect vision-language models with action execution systems"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"introduction-to-vision-language-models",children:"Introduction to Vision-Language Models"}),"\n",(0,t.jsx)(e.p,{children:"Vision-Language Models (VLMs) are neural networks that can process both visual and textual information simultaneously. In robotics, these models bridge the gap between high-level linguistic commands and low-level visual-motor control."}),"\n",(0,t.jsx)(e.h3,{id:"key-challenges-in-vision-language-integration",children:"Key Challenges in Vision-Language Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Grounded Language Understanding"}),": Connecting abstract language tokens to concrete visual features"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Spatial Reasoning"}),': Understanding spatial relationships in natural language (e.g., "move to the left of the red cup")']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Concept Grounding"}),": Learning to associate words with visual concepts in context"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Generalization"}),": Applying learned models to novel combinations of objects and actions"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"foundation-models-for-robotics",children:"Foundation Models for Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Foundation models like CLIP, PaLI, and BLIP-2 have revolutionized computer vision and NLP by providing large pre-trained models that can be adapted to downstream tasks."}),"\n",(0,t.jsx)(e.h3,{id:"contrastive-language-image-pre-training-clip",children:"Contrastive Language-Image Pre-training (CLIP)"}),"\n",(0,t.jsx)(e.p,{children:"CLIP trains an image encoder and a text encoder to map semantically similar images and text to nearby representations in a shared embedding space:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport clip\r\nfrom PIL import Image\r\n\r\n# Example of using a pre-trained CLIP model\r\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\r\nmodel, preprocess = clip.load("ViT-B/32", device=device)\r\n\r\n# Load image and text\r\nimage = preprocess(Image.open("robot_scene.jpg")).unsqueeze(0).to(device)\r\ntext = clip.tokenize(["robot moving box", "robot grasping object", "robot standing still"]).to(device)\r\n\r\n# Get similarities\r\nwith torch.no_grad():\r\n    image_features = model.encode_image(image)\r\n    text_features = model.encode_text(text)\r\n    \r\n    logits_per_image, logits_per_text = model(image, text)\r\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\r\n\r\nprint("Label probabilities:", probs)  # Prints: [[0.03, 0.95, 0.02]]\n'})}),"\n",(0,t.jsx)(e.h3,{id:"robotics-specific-adaptations",children:"Robotics-Specific Adaptations"}),"\n",(0,t.jsx)(e.p,{children:"While general VLMs provide strong foundational capabilities, robotics-specific adaptations are needed:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Integration"}),": Incorporating temporal sequences for action recognition and prediction"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Embodied Learning"}),": Training on datasets that include robot interactions with the physical world"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fine-grained Recognition"}),": Understanding subtle differences that matter for task execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"3D Awareness"}),": Extending from 2D images to 3D understanding for manipulation"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"implementing-vision-language-integration",children:"Implementing Vision-Language Integration"}),"\n",(0,t.jsx)(e.p,{children:"Let's implement a vision-language system for robotic task understanding:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as T\r\nfrom transformers import CLIPModel, CLIPProcessor\r\nimport numpy as np\r\nfrom typing import List, Dict, Tuple\r\n\r\nclass VisionLanguageRobotModule(nn.Module):\r\n    """\r\n    A vision-language module for robotic task understanding.\r\n    Combines visual perception with language processing for action planning.\r\n    """\r\n    def __init__(\r\n        self,\r\n        vision_encoder: nn.Module,\r\n        language_encoder: nn.Module,\r\n        fusion_dim: int = 512,\r\n        action_space_dim: int = 7  # For 7-DOF robotic arm\r\n    ):\r\n        super().__init__()\r\n        \r\n        self.vision_encoder = vision_encoder\r\n        self.language_encoder = language_encoder\r\n        \r\n        # Fusion layer to combine vision and language embeddings\r\n        self.fusion_layer = nn.Sequential(\r\n            nn.Linear(fusion_dim * 2, fusion_dim * 2),\r\n            nn.ReLU(),\r\n            nn.Linear(fusion_dim * 2, fusion_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        # Action prediction head\r\n        self.action_head = nn.Sequential(\r\n            nn.Linear(fusion_dim, fusion_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Linear(fusion_dim // 2, action_space_dim)\r\n        )\r\n        \r\n        # Task prediction head\r\n        self.task_head = nn.Sequential(\r\n            nn.Linear(fusion_dim, fusion_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Linear(fusion_dim // 2, 100)  # Assuming 100 possible tasks\r\n        )\r\n\r\n    def forward(self, images: torch.Tensor, texts: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        """\r\n        Forward pass through the vision-language module.\r\n        \r\n        Args:\r\n            images: Batch of images (B, C, H, W)\r\n            texts: List of text prompts (B,)\r\n        \r\n        Returns:\r\n            action_predictions: Predicted actions (B, action_space_dim)\r\n            task_predictions: Task classification scores (B, num_tasks)\r\n        """\r\n        # Encode visual information\r\n        visual_features = self.vision_encoder(images)\r\n        \r\n        # Encode textual information (simplified, in practice use tokenizer and text encoder)\r\n        text_features = self.encode_texts(texts)\r\n        \r\n        # Fuse vision and language representations\r\n        fused_features = self.fusion_layer(\r\n            torch.cat([visual_features, text_features], dim=1)\r\n        )\r\n        \r\n        # Predict actions and tasks\r\n        action_predictions = self.action_head(fused_features)\r\n        task_predictions = self.task_head(fused_features)\r\n        \r\n        return action_predictions, task_predictions\r\n\r\n    def encode_texts(self, texts: List[str]) -> torch.Tensor:\r\n        """\r\n        Simplified text encoding. In practice, this would use a pre-trained language model.\r\n        """\r\n        # Placeholder implementation - in practice, use a transformer-based encoder\r\n        # and tokenize the input texts\r\n        batch_size = len(texts)\r\n        # Return dummy features (in practice, encode texts using RoBERTa, BERT, etc.)\r\n        return torch.randn(batch_size, self.fusion_dim).to(next(self.parameters()).device)\r\n\r\n# Example usage\r\ndef create_robot_vlm() -> VisionLanguageRobotModule:\r\n    """\r\n    Create a vision-language model for robotic applications.\r\n    In practice, this would use pre-trained components from models like CLIP or PaLI.\r\n    """\r\n    # In practice, use pre-trained encoders\r\n    vision_encoder = nn.Conv2d(3, 512, kernel_size=3, stride=2, padding=1)  # Simplified\r\n    language_encoder = nn.Embedding(10000, 512)  # Simplified\r\n    \r\n    return VisionLanguageRobotModule(\r\n        vision_encoder=vision_encoder,\r\n        language_encoder=language_encoder,\r\n        fusion_dim=512\r\n    )\r\n\r\n# Usage example\r\nrobot_vlm = create_robot_vlm()\r\ndummy_images = torch.randn(4, 3, 224, 224)  # Batch of 4 images\r\ndummy_texts = [\r\n    "move the red block to the left",\r\n    "pick up the pencil",\r\n    "rotate the object 90 degrees",\r\n    "move forward and grasp"\r\n]\r\n\r\nactions, tasks = robot_vlm(dummy_images, dummy_texts)\r\nprint(f"Action predictions shape: {actions.shape}")\r\nprint(f"Task predictions shape: {tasks.shape}")\n'})}),"\n",(0,t.jsx)(e.h2,{id:"multimodal-fusion-techniques",children:"Multimodal Fusion Techniques"}),"\n",(0,t.jsx)(e.p,{children:"Effective vision-language integration requires sophisticated fusion techniques:"}),"\n",(0,t.jsx)(e.h3,{id:"early-fusion",children:"Early Fusion"}),"\n",(0,t.jsx)(e.p,{children:"Combining modalities at the input level by concatenating features early in the network:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class EarlyFusionModule(nn.Module):\r\n    """\r\n    Example of early fusion for vision-language integration.\r\n    Concatenates visual and textual features early in the processing pipeline.\r\n    """\r\n    def __init__(self, vis_dim: int, lang_dim: int, output_dim: int):\r\n        super().__init__()\r\n        self.projection = nn.Linear(vis_dim + lang_dim, output_dim)\r\n        self.norm = nn.LayerNorm(output_dim)\r\n        self.activation = nn.ReLU()\r\n        \r\n    def forward(self, vision_features: torch.Tensor, \r\n                language_features: torch.Tensor) -> torch.Tensor:\r\n        # Concatenate vision and language features\r\n        concatenated = torch.cat([vision_features, language_features], dim=-1)\r\n        \r\n        # Project to output dimension\r\n        fused = self.projection(concatenated)\r\n        \r\n        # Apply normalization and activation\r\n        return self.activation(self.norm(fused))\n'})}),"\n",(0,t.jsx)(e.h3,{id:"late-fusion",children:"Late Fusion"}),"\n",(0,t.jsx)(e.p,{children:"Processing modalities independently before combining at a later stage:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class LateFusionModule(nn.Module):\r\n    """\r\n    Example of late fusion for vision-language integration.\r\n    Processes modalities independently before fusing.\r\n    """\r\n    def __init__(self, common_dim: int):\r\n        super().__init__()\r\n        self.vis_projection = nn.Linear(512, common_dim)  # Vision to common space\r\n        self.lang_projection = nn.Linear(512, common_dim)  # Language to common space\r\n        self.fusion_gate = nn.Linear(common_dim * 2, 1)\r\n        \r\n    def forward(self, vision_features: torch.Tensor, \r\n                language_features: torch.Tensor) -> torch.Tensor:\r\n        # Project both modalities to common space\r\n        projected_vis = self.vis_projection(vision_features)\r\n        projected_lang = self.lang_projection(language_features)\r\n        \r\n        # Learnable weighted combination\r\n        combined = torch.cat([projected_vis, projected_lang], dim=-1)\r\n        gate_weights = torch.sigmoid(self.fusion_gate(combined))\r\n        \r\n        # Weighted sum\r\n        return gate_weights * projected_vis + (1 - gate_weights) * projected_lang\n'})}),"\n",(0,t.jsx)(e.h3,{id:"cross-attention-fusion",children:"Cross-Attention Fusion"}),"\n",(0,t.jsx)(e.p,{children:"Using attention mechanisms to allow each modality to attend to relevant parts of the other:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class CrossAttentionFusion(nn.Module):\r\n    """\r\n    Cross-attention fusion for vision-language integration.\r\n    Allows each modality to attend to relevant parts of the other.\r\n    """\r\n    def __init__(self, hidden_dim: int, num_heads: int = 8):\r\n        super().__init__()\r\n        self.hidden_dim = hidden_dim\r\n        self.num_heads = num_heads\r\n        self.head_dim = hidden_dim // num_heads\r\n        \r\n        # Projections for vision and language\r\n        self.vis_q = nn.Linear(hidden_dim, hidden_dim)\r\n        self.vis_k = nn.Linear(hidden_dim, hidden_dim)\r\n        self.vis_v = nn.Linear(hidden_dim, hidden_dim)\r\n        \r\n        self.lang_q = nn.Linear(hidden_dim, hidden_dim)\r\n        self.lang_k = nn.Linear(hidden_dim, hidden_dim)\r\n        self.lang_v = nn.Linear(hidden_dim, hidden_dim)\r\n        \r\n        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\r\n        \r\n    def forward(self, vision_features: torch.Tensor, \r\n                language_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        # Cross-attention: vision attending to language\r\n        lang_q = self.lang_q(language_features)\r\n        vis_k = self.vis_k(vision_features)\r\n        vis_v = self.vis_v(vision_features)\r\n        \r\n        # Compute attention weights\r\n        attn_weights_v2l = torch.softmax(\r\n            torch.matmul(lang_q, vis_k.transpose(-2, -1)) / (self.head_dim ** 0.5),\r\n            dim=-1\r\n        )\r\n        \r\n        # Apply attention\r\n        attended_language = torch.matmul(attn_weights_v2l, vis_v)\r\n        \r\n        # Cross-attention: language attending to vision\r\n        vis_q = self.vis_q(vision_features)\r\n        lang_k = self.lang_k(language_features)\r\n        lang_v = self.lang_v(language_features)\r\n        \r\n        attn_weights_l2v = torch.softmax(\r\n            torch.matmul(vis_q, lang_k.transpose(-2, -1)) / (self.head_dim ** 0.5),\r\n            dim=-1\r\n        )\r\n        \r\n        attended_vision = torch.matmul(attn_weights_l2v, lang_v)\r\n        \r\n        # Output projections\r\n        fused_vision = self.out_proj(attended_vision)\r\n        fused_language = self.out_proj(attended_language)\r\n        \r\n        return fused_vision, fused_language\n'})}),"\n",(0,t.jsx)(e.h2,{id:"vision-language-datasets-for-robotics",children:"Vision-Language Datasets for Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Training effective vision-language models for robotics requires appropriate datasets:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ALOHA Dataset"}),": Large-scale dataset of robot manipulation demonstrations with language descriptions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Bridge Data"}),": Dataset of robot tasks performed by human operators with language annotations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robotics Transformer 2 (RT-2) Dataset"}),": Web-scale pre-training data combined with robotic experience"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"OXE (Open X-Embodiment)"}),": Collection of diverse robotic datasets for cross-embodiment learning"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"transfer-learning-with-pre-trained-models",children:"Transfer Learning with Pre-trained Models"}),"\n",(0,t.jsx)(e.p,{children:"A common approach is to adapt pre-trained vision-language models to robotics:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nfrom transformers import CLIPModel, CLIPProcessor\r\nfrom peft import get_peft_model, LoraConfig  # Parameter-efficient fine-tuning\r\n\r\nclass FineTunedRobotVLM(nn.Module):\r\n    """\r\n    A robot-specific vision-language model adapted from a pre-trained foundation model.\r\n    Uses parameter-efficient fine-tuning to adapt to robotic tasks.\r\n    """\r\n    def __init__(self, base_model_name: str = "openai/clip-vit-base-patch32"):\r\n        super().__init__()\r\n        \r\n        # Load pre-trained model\r\n        self.base_model = CLIPModel.from_pretrained(base_model_name)\r\n        \r\n        # Freeze base model weights (optional - depends on training strategy)\r\n        for param in self.base_model.parameters():\r\n            param.requires_grad = False\r\n            \r\n        # Add robotic-specific components\r\n        self.robot_task_head = nn.Sequential(\r\n            nn.Linear(self.base_model.config.projection_dim, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 10)  # 10 robotic tasks as example\r\n        )\r\n        \r\n        # Action prediction head\r\n        self.action_predictor = nn.Sequential(\r\n            nn.Linear(self.base_model.config.projection_dim, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 7)  # 7-DOF robot arm actions\r\n        )\r\n\r\n    def forward(self, images: torch.Tensor, texts: torch.Tensor):\r\n        """\r\n        Forward pass with pre-trained encoder and robot-specific heads.\r\n        """\r\n        # Get encodings from frozen pre-trained model\r\n        outputs = self.base_model(pixel_values=images, input_ids=texts)\r\n        \r\n        # Extract joint vision-language embedding\r\n        joint_embedding = outputs.logits_per_image  # Shape: (batch_size, batch_size)\r\n        \r\n        # Predict robot-specific outputs\r\n        task_logits = self.robot_task_head(joint_embedding)\r\n        action_pred = self.action_predictor(joint_embedding)\r\n        \r\n        return task_logits, action_pred\r\n\r\n# Example usage\r\nrobot_model = FineTunedRobotVLM()\r\n# Further training would involve robot-specific datasets\n'})}),"\n",(0,t.jsx)(e.h2,{id:"practical-applications",children:"Practical Applications"}),"\n",(0,t.jsx)(e.p,{children:"Vision-language integration enables several key capabilities in robotics:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Command Following"}),": Interpreting natural language commands to navigate or manipulate"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Semantic Scene Understanding"}),": Identifying and reasoning about objects and their affordances"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Generalization"}),": Applying learned concepts to novel combinations of objects and verbs"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Interactive Learning"}),": Correcting robot behavior through natural language feedback"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"lab-exercise-preview",children:"Lab Exercise Preview"}),"\n",(0,t.jsx)(e.p,{children:"In the next section, you'll find detailed instructions for implementing a vision-language model for a specific robotic task, including data preparation, model fine-tuning, and evaluation metrics."}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"In these weeks, you've learned:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"How vision-language models connect abstract language to concrete visual perception"}),"\n",(0,t.jsx)(e.li,{children:"Different fusion techniques for combining vision and language modalities"}),"\n",(0,t.jsx)(e.li,{children:"How to adapt pre-trained foundation models for robotic applications"}),"\n",(0,t.jsx)(e.li,{children:"Practical applications of vision-language integration in robotics"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:["Continue to ",(0,t.jsx)(e.a,{href:"/my-website/vla/week12",children:"Week 12: Action Planning with LLMs"})," to explore how language models can generate action plans for robotic systems."]})]})}function g(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>s});var r=i(6540);const t={},a=r.createContext(t);function o(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);