"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[2009],{5458:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"vla/assessments/assignment1","title":"Assignment 1 - Implementing a Vision-Language-Action Pipeline","description":"Objective","source":"@site/docs/vla/assessments/assignment1.md","sourceDirName":"vla/assessments","slug":"/vla/assessments/assignment1","permalink":"/vla/assessments/assignment1","draft":false,"unlisted":false,"editUrl":"https://github.com/Bushraturk/Physical-AI-Book/edit/main/docs/vla/assessments/assignment1.md","tags":[],"version":"current","sidebarPosition":23,"frontMatter":{"title":"Assignment 1 - Implementing a Vision-Language-Action Pipeline","sidebar_position":23},"sidebar":"textbookSidebar","previous":{"title":"Quiz 1 - Vision-Language-Action Fundamentals","permalink":"/vla/assessments/quiz1"},"next":{"title":"VLA Module Conclusion","permalink":"/vla/conclusion"}}');var t=i(4848),r=i(8453);const l={title:"Assignment 1 - Implementing a Vision-Language-Action Pipeline",sidebar_position:23},o="Assignment 1: Implementing a Vision-Language-Action Pipeline",a={},c=[{value:"Objective",id:"objective",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Assignment Requirements",id:"assignment-requirements",level:2},{value:"Technical Specifications",id:"technical-specifications",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Core Components to Implement",id:"core-components-to-implement",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Phase 1: System Design (20 points)",id:"phase-1-system-design-20-points",level:3},{value:"Phase 2: Component Implementation (50 points)",id:"phase-2-component-implementation-50-points",level:3},{value:"Phase 3: System Integration (20 points)",id:"phase-3-system-integration-20-points",level:3},{value:"Phase 4: Evaluation (10 points)",id:"phase-4-evaluation-10-points",level:3},{value:"Code Requirements",id:"code-requirements",level:2},{value:"Example ROS 2 Node Structure",id:"example-ros-2-node-structure",level:3},{value:"Testing Requirements",id:"testing-requirements",level:2},{value:"Deliverables",id:"deliverables",level:2},{value:"Grading Rubric",id:"grading-rubric",level:2},{value:"Performance Benchmarks",id:"performance-benchmarks",level:3},{value:"Extra Credit Opportunities (Up to 10 bonus points)",id:"extra-credit-opportunities-up-to-10-bonus-points",level:2},{value:"Resources",id:"resources",level:2},{value:"Submission Guidelines",id:"submission-guidelines",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"assignment-1-implementing-a-vision-language-action-pipeline",children:"Assignment 1: Implementing a Vision-Language-Action Pipeline"})}),"\n",(0,t.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Implement a complete Vision-Language-Action (VLA) pipeline that can interpret natural language commands, perceive the environment through vision systems, and execute appropriate robotic actions. This assignment integrates all aspects of the VLA module with previous course modules."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of VLA module content (Weeks 10-13)"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of ROS 2 concepts (Module 1)"}),"\n",(0,t.jsx)(n.li,{children:"Experience with simulation environments (Module 2)"}),"\n",(0,t.jsx)(n.li,{children:"Knowledge of NVIDIA Isaac and perception (Module 3)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"assignment-requirements",children:"Assignment Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Your VLA pipeline must include:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Understanding Module"}),": Process natural language commands and extract actionable information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision Processing Module"}),": Analyze visual input to understand the environment and objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Planning Module"}),": Generate executable actions based on language commands and visual input"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution Module"}),": Interface with robotic systems to execute planned actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration"}),": All modules must work together in a cohesive system"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"technical-specifications",children:"Technical Specifications"}),"\n",(0,t.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Your system should follow a modular architecture with clear interfaces between components:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[Language Input] -> [Language Understanding] -> [Action Planner]\r\n                     |                             |\r\n                     v                             v\r\n[Visual Input] -> [Vision Processing] -> [Action Executor] -> [Robot]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"core-components-to-implement",children:"Core Components to Implement"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Language Parser"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Accept natural language commands"}),"\n",(0,t.jsx)(n.li,{children:"Identify task objectives, target objects, and spatial relationships"}),"\n",(0,t.jsx)(n.li,{children:"Generate structured action requests"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Perception System"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Process camera input for object detection and localization"}),"\n",(0,t.jsx)(n.li,{children:"Integrate Isaac ROS perception packages"}),"\n",(0,t.jsx)(n.li,{children:"Maintain spatial map of objects in the environment"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Action Generator"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Convert high-level objectives into sequences of robot actions"}),"\n",(0,t.jsx)(n.li,{children:"Integrate navigation and manipulation primitives"}),"\n",(0,t.jsx)(n.li,{children:"Handle error recovery and replanning"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Execution Coordinator"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Manage the execution of action sequences"}),"\n",(0,t.jsx)(n.li,{children:"Monitor progress and adjust plans as needed"}),"\n",(0,t.jsx)(n.li,{children:"Provide feedback on task completion status"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,t.jsx)(n.h3,{id:"phase-1-system-design-20-points",children:"Phase 1: System Design (20 points)"}),"\n",(0,t.jsx)(n.p,{children:"Create a detailed system architecture document that includes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Block diagram showing component interactions"}),"\n",(0,t.jsx)(n.li,{children:"Data flow between modules"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 message types and topics"}),"\n",(0,t.jsx)(n.li,{children:"Error handling mechanisms"}),"\n",(0,t.jsx)(n.li,{children:"Performance requirements and constraints"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Submit a 2-3 page design document with architectural diagrams."}),"\n",(0,t.jsx)(n.h3,{id:"phase-2-component-implementation-50-points",children:"Phase 2: Component Implementation (50 points)"}),"\n",(0,t.jsx)(n.p,{children:"Implement each of the core components:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Language Understanding Module (15 points)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create a ROS 2 node that processes natural language commands"}),"\n",(0,t.jsx)(n.li,{children:"Use appropriate NLP techniques to extract intent and entities"}),"\n",(0,t.jsx)(n.li,{children:"Generate structured action requests based on the input"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Vision Processing Module (15 points)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate Isaac ROS perception components"}),"\n",(0,t.jsx)(n.li,{children:"Detect and localize objects relevant to tasks"}),"\n",(0,t.jsx)(n.li,{children:"Maintain spatial relationships between objects"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Action Planning and Execution Module (20 points)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Plan sequences of actions based on command and perception inputs"}),"\n",(0,t.jsx)(n.li,{children:"Integrate with robot control systems (simulated or real)"}),"\n",(0,t.jsx)(n.li,{children:"Implement feedback and error recovery mechanisms"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"phase-3-system-integration-20-points",children:"Phase 3: System Integration (20 points)"}),"\n",(0,t.jsx)(n.p,{children:"Connect all components into a unified system:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ensure proper data flow between modules"}),"\n",(0,t.jsx)(n.li,{children:"Handle asynchronous operations appropriately"}),"\n",(0,t.jsx)(n.li,{children:"Implement system-level error handling"}),"\n",(0,t.jsx)(n.li,{children:"Test integration in simulation environment"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"phase-4-evaluation-10-points",children:"Phase 4: Evaluation (10 points)"}),"\n",(0,t.jsx)(n.p,{children:"Evaluate your system's performance:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Test with at least 5 different command types"}),"\n",(0,t.jsx)(n.li,{children:"Measure success rate and execution time"}),"\n",(0,t.jsx)(n.li,{children:"Document limitations and potential improvements"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"code-requirements",children:"Code Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Your implementation must:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Follow ROS 2 best practices and conventions"}),"\n",(0,t.jsx)(n.li,{children:"Include comprehensive documentation"}),"\n",(0,t.jsx)(n.li,{children:"Have appropriate error handling"}),"\n",(0,t.jsx)(n.li,{children:"Be modular and maintainable"}),"\n",(0,t.jsx)(n.li,{children:"Include unit tests for critical components"}),"\n",(0,t.jsx)(n.li,{children:"Use Isaac ROS packages for perception where applicable"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-ros-2-node-structure",children:"Example ROS 2 Node Structure"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Pose\r\nfrom vla_msgs.msg import ActionRequest, ActionResult  # You\'ll need to define this message type\r\n\r\n\r\nclass VLAPipelineNode(Node):\r\n    """\r\n    Main node for the Vision-Language-Action pipeline.\r\n    Coordinates the interaction between language, vision, and action components.\r\n    """\r\n    def __init__(self):\r\n        super().__init__(\'vla_pipeline\')\r\n        \r\n        # Subscriptions\r\n        self.command_subscriber = self.create_subscription(\r\n            String,\r\n            \'natural_language_command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n        self.image_subscriber = self.create_subscription(\r\n            Image,\r\n            \'camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.action_publisher = self.create_publisher(\r\n            ActionRequest,\r\n            \'action_requests\',\r\n            10\r\n        )\r\n        \r\n        self.status_publisher = self.create_publisher(\r\n            String,\r\n            \'vla_pipeline/status\',\r\n            10\r\n        )\r\n        \r\n        # Internal state\r\n        self.latest_image = None\r\n        self.perception_results = {}\r\n        self.language_understanding = None\r\n        \r\n        self.get_logger().info("VLA Pipeline initialized")\r\n\r\n    def command_callback(self, msg):\r\n        """Process natural language command and initiate action planning."""\r\n        command = msg.data\r\n        self.get_logger().info(f"Received command: {command}")\r\n        \r\n        # Step 1: Use language understanding component\r\n        action_request = self.process_language_command(command)\r\n        \r\n        if action_request:\r\n            # Step 2: Incorporate perception data\r\n            enhanced_request = self.enhance_with_perception(action_request)\r\n            \r\n            # Step 3: Publish action request\r\n            self.action_publisher.publish(enhanced_request)\r\n            \r\n            # Step 4: Monitor execution and provide feedback\r\n            self.monitor_execution(enhanced_request.id)\r\n        else:\r\n            self.get_logger().error(f"Could not process command: {command}")\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming image for environmental perception."""\r\n        # Process image with Isaac perception components\r\n        # Store relevant information for action planning\r\n        pass\r\n\r\n    def process_language_command(self, command: str) -> ActionRequest:\r\n        """\r\n        Process natural language command and generate initial action request.\r\n        \r\n        Args:\r\n            command: Natural language command string\r\n            \r\n        Returns:\r\n            ActionRequest message with initial plan\r\n        """\r\n        # In a real implementation, this would use NLP/VLA models\r\n        # For this assignment, implement a rule-based parser or simple NLP approach\r\n        pass\r\n\r\n    def enhance_with_perception(self, action_request: ActionRequest) -> ActionRequest:\r\n        """\r\n        Enhance action request with perception data.\r\n        \r\n        Args:\r\n            action_request: Initial action request from language processing\r\n            \r\n        Returns:\r\n            ActionRequest enhanced with perceptual information\r\n        """\r\n        # Use perception data to refine action plan\r\n        # Add object locations, spatial relationships, etc.\r\n        pass\r\n\r\n    def monitor_execution(self, request_id: str):\r\n        """Monitor the execution of a specific action request."""\r\n        # Monitor action execution and provide feedback\r\n        pass\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    \r\n    vla_pipeline = VLAPipelineNode()\r\n    \r\n    try:\r\n        rclpy.spin(vla_pipeline)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        vla_pipeline.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"testing-requirements",children:"Testing Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Create test cases that demonstrate your system's capabilities:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simple Navigation"}),': "Go to the red cube"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Manipulation"}),': "Pick up the blue cup"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Spatial Relations"}),': "Go to the left of the chair"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sequential Tasks"}),': "Go to the table and pick up the book"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conditional Execution"}),': "If you see a green ball, go to it"']}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"deliverables",children:"Deliverables"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Source Code"}),": Complete, documented implementation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Design Document"}),": 2-3 pages with architecture diagrams"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test Results"}),": Documentation of system performance on test cases"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Video Demonstration"}),": Short video showing system in action (simulation or real robot)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reflection Report"}),": 1-2 pages discussing challenges, solutions, and lessons learned"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"System Design Document (20 points)"}),"\n",(0,t.jsxs)(n.li,{children:["Component Implementation (50 points)","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Language Understanding (15 points)"}),"\n",(0,t.jsx)(n.li,{children:"Vision Processing (15 points)"}),"\n",(0,t.jsx)(n.li,{children:"Action Planning/Execution (20 points)"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"System Integration (20 points)"}),"\n",(0,t.jsx)(n.li,{children:"Evaluation and Testing (10 points)"}),"\n",(0,t.jsx)(n.li,{children:"Total: 100 points"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,t.jsx)(n.p,{children:"For full credit on implementation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Language Understanding: 80% accuracy on simple commands"}),"\n",(0,t.jsx)(n.li,{children:"Vision Processing: Successfully detect and localize target objects in 80% of attempts"}),"\n",(0,t.jsx)(n.li,{children:"Action Execution: Complete 70% of attempted tasks successfully"}),"\n",(0,t.jsx)(n.li,{children:"System Integration: All components communicate effectively without crashes"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"extra-credit-opportunities-up-to-10-bonus-points",children:"Extra Credit Opportunities (Up to 10 bonus points)"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement a learning component that improves performance based on feedback"}),"\n",(0,t.jsx)(n.li,{children:"Extend the system to handle multi-modal inputs (e.g., pointing + language)"}),"\n",(0,t.jsx)(n.li,{children:"Add uncertainty awareness to the action planning component"}),"\n",(0,t.jsx)(n.li,{children:"Implement sim-to-real transfer capabilities"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/",children:"NVIDIA Isaac ROS Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ros.org/",children:"ROS 2 Documentation"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/index",children:"Hugging Face Transformers"})," for NLP components"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2103.00020",children:"CLIP Paper"})," for vision-language understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://vila-project.github.io/",children:"OpenVLA Project"})," for state-of-the-art VLA approaches"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"submission-guidelines",children:"Submission Guidelines"}),"\n",(0,t.jsx)(n.p,{children:"Submit your assignment as a ZIP file containing:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["A ",(0,t.jsx)(n.code,{children:"src/"})," directory with all source code"]}),"\n",(0,t.jsxs)(n.li,{children:["A ",(0,t.jsx)(n.code,{children:"doc/"})," directory with your design document and reflection report"]}),"\n",(0,t.jsxs)(n.li,{children:["A ",(0,t.jsx)(n.code,{children:"launch/"})," directory with launch files for your system"]}),"\n",(0,t.jsxs)(n.li,{children:["A ",(0,t.jsx)(n.code,{children:"test/"})," directory with test cases and results"]}),"\n",(0,t.jsx)(n.li,{children:"A README.md file with setup and usage instructions"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Name your submission as ",(0,t.jsx)(n.code,{children:"vla_assignment_lastname_firstname.zip"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Due Date: [Specify date per course schedule]\r\nLate submissions will incur a 5% penalty per day."})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function l(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);