"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[73],{4697:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"nvidia-isaac/lab-exercises/lab1","title":"Lab Exercise 1 - NVIDIA Isaac Perception and VSLAM Implementation","description":"Objective","source":"@site/docs/nvidia-isaac/lab-exercises/lab1.md","sourceDirName":"nvidia-isaac/lab-exercises","slug":"/nvidia-isaac/lab-exercises/lab1","permalink":"/nvidia-isaac/lab-exercises/lab1","draft":false,"unlisted":false,"editUrl":"https://github.com/Bushraturk/Physical-AI-Book/edit/main/docs/nvidia-isaac/lab-exercises/lab1.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"title":"Lab Exercise 1 - NVIDIA Isaac Perception and VSLAM Implementation","sidebar_position":12},"sidebar":"textbookSidebar","previous":{"title":"Week 9 - AI-Robot Brain Integration and Reinforcement Learning","permalink":"/nvidia-isaac/week9"},"next":{"title":"Quiz 1 - NVIDIA Isaac Perception and AI Fundamentals","permalink":"/nvidia-isaac/assessments/quiz1"}}');var t=r(4848),a=r(8453);const s={title:"Lab Exercise 1 - NVIDIA Isaac Perception and VSLAM Implementation",sidebar_position:12},o="Lab Exercise 1: NVIDIA Isaac Perception and VSLAM Implementation",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Equipment Required",id:"equipment-required",level:2},{value:"Lab Steps",id:"lab-steps",level:2},{value:"Step 1: Environment Setup",id:"step-1-environment-setup",level:3},{value:"Step 2: Create the Perception Pipeline Node",id:"step-2-create-the-perception-pipeline-node",level:3},{value:"Step 3: Implement a Reinforcement Learning Component",id:"step-3-implement-a-reinforcement-learning-component",level:3},{value:"Step 4: Running the Complete System",id:"step-4-running-the-complete-system",level:3},{value:"Step 5: Testing and Evaluation",id:"step-5-testing-and-evaluation",level:3},{value:"Expected Results",id:"expected-results",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Extension Activities",id:"extension-activities",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lab-exercise-1-nvidia-isaac-perception-and-vslam-implementation",children:"Lab Exercise 1: NVIDIA Isaac Perception and VSLAM Implementation"})}),"\n",(0,t.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"In this lab exercise, you will implement a complete perception and visual SLAM pipeline using NVIDIA Isaac SDK. You'll create a system that processes camera data to detect objects, estimate depth, and build a map of the environment while the robot navigates."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this lab, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Set up NVIDIA Isaac perception pipeline with optimized deep learning models"}),"\n",(0,t.jsx)(n.li,{children:"Implement Visual SLAM to map the environment and track robot position"}),"\n",(0,t.jsx)(n.li,{children:"Integrate perception with navigation and control systems"}),"\n",(0,t.jsx)(n.li,{children:"Deploy AI models on edge computing platforms (e.g., NVIDIA Jetson)"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate and tune perception algorithms for your specific robot platform"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of NVIDIA Isaac Modules 7-9 content"}),"\n",(0,t.jsx)(n.li,{children:"Access to NVIDIA GPU or Jetson development kit"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of ROS 2 concepts from Module 1"}),"\n",(0,t.jsx)(n.li,{children:"Basic Python and C++ programming skills"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with computer vision concepts"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"equipment-required",children:"Equipment Required"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"NVIDIA Jetson platform (Xavier NX, Orin, or equivalent) or RTX workstation"}),"\n",(0,t.jsx)(n.li,{children:"RGB camera with calibration parameters"}),"\n",(0,t.jsx)(n.li,{children:"Robot platform capable of differential drive control"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2-enabled robot controller"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"lab-steps",children:"Lab Steps"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"NVIDIA Isaac Perception and VSLAM Architecture",src:r(9161).A+"",width:"800",height:"600"})}),"\n",(0,t.jsx)(n.h3,{id:"step-1-environment-setup",children:"Step 1: Environment Setup"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Verify your NVIDIA Isaac and ROS 2 integration:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Verify Isaac ROS packages are installed\r\ndpkg -l | grep isaac\r\n\r\n# Source ROS 2 and Isaac environments\r\nsource /opt/ros/humble/setup.bash\r\nsource /opt/nvidia/isaac/ros_humble/latest/setup.bash\r\n\r\n# Check for Isaac ROS nodes\r\nros2 pkg list | grep isaac\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Create a new workspace for the perception pipeline:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/isaac_ws/src\r\ncd ~/isaac_ws/src\r\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\r\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_detection_postprocessor.git\r\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_unified_rectifier.git\r\n\r\ncd ~/isaac_ws\r\ncolcon build --symlink-install --packages-select \\\r\n  isaac_ros_visual_slam \\\r\n  isaac_ros_detection_postprocessor \\\r\n  isaac_ros_unified_rectifier\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-create-the-perception-pipeline-node",children:"Step 2: Create the Perception Pipeline Node"}),"\n",(0,t.jsx)(n.p,{children:"Create a new ROS 2 package for the perception system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ws/src\r\nros2 pkg create --build-type ament_python perception_pipeline\r\ncd perception_pipeline\r\nmkdir -p perception_pipeline/{launch,config}\n"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Create the main perception pipeline node ",(0,t.jsx)(n.code,{children:"perception_pipeline/perception_pipeline/main_perception.py"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav_msgs.msg import Odometry\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom builtin_interfaces.msg import Time\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.transforms as T\r\nfrom collections import deque\r\nimport tf_transformations\r\n\r\n\r\nclass IsaacPerceptionPipeline(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_perception_pipeline\')\r\n        \r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n        \r\n        # Initialize queues for temporal processing\r\n        self.image_queue = deque(maxlen=5)\r\n        \r\n        # Initialize pose tracking for VSLAM\r\n        self.current_pose = np.zeros(3)  # x, y, theta\r\n        self.current_orientation = np.eye(3)\r\n        \r\n        # Subscribe to camera topics\r\n        self.image_subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        self.info_subscription = self.create_subscription(\r\n            CameraInfo,\r\n            \'/camera/camera_info\',\r\n            self.info_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher for processed perception data\r\n        self.object_detection_publisher = self.create_publisher(\r\n            Detection2DArray,\r\n            \'/perception/detections\',\r\n            10\r\n        )\r\n        \r\n        self.vslam_publisher = self.create_publisher(\r\n            Odometry,\r\n            \'/vslam/odometry\',\r\n            10\r\n        )\r\n        \r\n        # Initialize camera parameters\r\n        self.camera_matrix = None\r\n        self.distortion_coeffs = None\r\n        \r\n        # Initialize deep learning models for perception\r\n        self.setup_deep_learning_models()\r\n        \r\n        self.get_logger().info("Isaac Perception Pipeline initialized")\r\n\r\n    def setup_deep_learning_models(self):\r\n        """\r\n        Initialize deep learning models for object detection and depth estimation\r\n        """\r\n        try:\r\n            # Load pre-trained detection model (e.g., using TorchVision)\r\n            self.detection_model = torch.hub.load(\r\n                \'ultralytics/yolov5\', \r\n                \'yolov5s\', \r\n                pretrained=True\r\n            )\r\n            self.detection_model.eval()\r\n            \r\n            # For a more Isaac-specific approach, you\'d load TensorRT optimized models\r\n            # self.detection_model = self.load_tensorrt_model("/path/to/optimized_model.plan")\r\n            \r\n            self.get_logger().info("Deep learning models loaded successfully")\r\n        except Exception as e:\r\n            self.get_logger().error(f"Failed to load deep learning models: {e}")\r\n\r\n    def info_callback(self, msg):\r\n        """Handle camera calibration information"""\r\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\r\n        self.distortion_coeffs = np.array(msg.d)\r\n\r\n    def image_callback(self, msg):\r\n        """Process incoming camera image for perception and VSLAM"""\r\n        try:\r\n            # Convert ROS image to OpenCV format\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\r\n            \r\n            # Add to queue for temporal processing\r\n            self.image_queue.append({\r\n                \'image\': cv_image,\r\n                \'timestamp\': msg.header.stamp\r\n            })\r\n            \r\n            # Run perception pipeline\r\n            detections = self.run_object_detection(cv_image)\r\n            vslam_result = self.run_visual_slam(cv_image, msg.header.stamp)\r\n            \r\n            # Publish results\r\n            if detections is not None:\r\n                self.publish_detections(detections, msg.header)\r\n            \r\n            if vslam_result is not None:\r\n                self.publish_vslam_result(vslam_result, msg.header)\r\n                \r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing image: {e}")\r\n\r\n    def run_object_detection(self, image):\r\n        """Run object detection on the input image"""\r\n        if self.detection_model is None:\r\n            return None\r\n            \r\n        try:\r\n            # Convert image for model input\r\n            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n            img_tensor = T.ToTensor()(img_rgb).unsqueeze(0)  # Add batch dimension\r\n            \r\n            # Run inference\r\n            with torch.no_grad():\r\n                results = self.detection_model(img_tensor)\r\n            \r\n            # Process results\r\n            # Note: This is a simplified approach. In Isaac, you\'d use optimized \r\n            # CUDA-accelerated models through Isaac ROS packages\r\n            detections = Detection2DArray()\r\n            \r\n            # Convert YOLO results to vision_msgs format\r\n            # This is a placeholder; actual implementation would depend on your model\r\n            for detection in results.xyxy[0]:  # xyxy format: [x1, y1, x2, y2, conf, class]\r\n                if detection[4] > 0.5:  # Confidence threshold\r\n                    # Process detection and add to result\r\n                    pass\r\n            \r\n            return results\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in object detection: {e}")\r\n            return None\r\n\r\n    def run_visual_slam(self, image, timestamp):\r\n        """Estimate camera motion using visual slam"""\r\n        if len(self.image_queue) < 2:\r\n            return None\r\n            \r\n        try:\r\n            # Get the previous image for comparison\r\n            prev_data = self.image_queue[-2]\r\n            curr_data = self.image_queue[-1]\r\n            \r\n            # Convert images to grayscale\r\n            prev_gray = cv2.cvtColor(prev_data[\'image\'], cv2.COLOR_BGR2GRAY)\r\n            curr_gray = cv2.cvtColor(curr_data[\'image\'], cv2.COLOR_BGR2GRAY)\r\n            \r\n            # Feature detection and matching (simplified approach)\r\n            # In Isaac, you\'d use optimized CUDA algorithms\r\n            orb = cv2.ORB_create(nfeatures=1000)\r\n            kp1, des1 = orb.detectAndCompute(prev_gray, None)\r\n            kp2, des2 = orb.detectAndCompute(curr_gray, None)\r\n            \r\n            if des1 is not None and des2 is not None:\r\n                bf = cv2.BFMatcher()\r\n                matches = bf.knnMatch(des1, des2, k=2)\r\n                \r\n                # Apply Lowe\'s ratio test\r\n                good_matches = []\r\n                for match_pair in matches:\r\n                    if len(match_pair) == 2:\r\n                        m, n = match_pair\r\n                        if m.distance < 0.75 * n.distance:\r\n                            good_matches.append(m)\r\n                \r\n                if len(good_matches) >= 10:  # Minimum matches required\r\n                    # Extract matched keypoints\r\n                    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\r\n                    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\r\n                    \r\n                    # Estimate essential matrix if camera matrix is available\r\n                    if self.camera_matrix is not None:\r\n                        E, mask = cv2.findEssentialMat(\r\n                            src_pts, dst_pts, \r\n                            self.camera_matrix, \r\n                            method=cv2.RANSAC, \r\n                            prob=0.999, \r\n                            threshold=1.0\r\n                        )\r\n                        \r\n                        if E is not None:\r\n                            # Recover pose\r\n                            _, R, t, _ = cv2.recoverPose(E, src_pts, dst_pts, self.camera_matrix)\r\n                            \r\n                            # Update pose estimate (simplified)\r\n                            dt = 0.1  # Time delta between frames\r\n                            self.current_pose[:2] += dt * t.flatten()[:2]  # Update x, y\r\n                            self.current_pose[2] += np.arctan2(R[1, 0], R[0, 0])  # Update theta\r\n                        \r\n                            return {\r\n                                \'position\': self.current_pose.copy(),\r\n                                \'orientation\': R.copy(),\r\n                                \'timestamp\': timestamp\r\n                            }\r\n            \r\n            return None\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in VSLAM: {e}")\r\n            return None\r\n\r\n    def publish_detections(self, detections, header):\r\n        """Publish object detection results"""\r\n        # This method would convert the detection results to the appropriate ROS message format\r\n        # For now, it\'s a placeholder\r\n        pass\r\n\r\n    def publish_vslam_result(self, vslam_result, header):\r\n        """Publish VSLAM results as odometry"""\r\n        odom_msg = Odometry()\r\n        odom_msg.header = header\r\n        odom_msg.header.frame_id = "odom"\r\n        odom_msg.child_frame_id = "base_link"\r\n        \r\n        # Set position\r\n        odom_msg.pose.pose.position.x = float(vslam_result[\'position\'][0])\r\n        odom_msg.pose.pose.position.y = float(vslam_result[\'position\'][1])\r\n        odom_msg.pose.pose.position.z = 0.0  # Assuming planar navigation\r\n        \r\n        # Convert orientation matrix to quaternion\r\n        R = vslam_result[\'orientation\']\r\n        qw, qx, qy, qz = tf_transformations.quaternion_from_matrix(\r\n            np.block([[R, [[0], [0], [0]]], [[0, 0, 0, 1]]])\r\n        )\r\n        \r\n        odom_msg.pose.pose.orientation.w = qw\r\n        odom_msg.pose.pose.orientation.x = qx\r\n        odom_msg.pose.pose.orientation.y = qy\r\n        odom_msg.pose.pose.orientation.z = qz\r\n        \r\n        # Publishing velocity estimates would require more sophisticated tracking\r\n        self.vslam_publisher.publish(odom_msg)\r\n\r\n    def load_tensorrt_model(self, model_path):\r\n        """\r\n        Load a TensorRT optimized model (placeholder implementation)\r\n        """\r\n        # This would use TensorRT Python bindings to load an optimized model\r\n        # In practice, you\'d use Isaac ROS packages that handle this efficiently\r\n        pass\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    \r\n    perception_pipeline = IsaacPerceptionPipeline()\r\n    \r\n    try:\r\n        rclpy.spin(perception_pipeline)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        perception_pipeline.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["Create a launch file ",(0,t.jsx)(n.code,{children:"perception_pipeline/launch/perception_pipeline_launch.py"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import Node\r\n\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    namespace_arg = DeclareLaunchArgument(\r\n        'namespace',\r\n        default_value='',\r\n        description='Namespace for the perception pipeline'\r\n    )\r\n    \r\n    # Create the perception pipeline node\r\n    perception_node = Node(\r\n        package='perception_pipeline',\r\n        executable='main_perception.py',\r\n        name='isaac_perception_pipeline',\r\n        namespace=LaunchConfiguration('namespace'),\r\n        parameters=[\r\n            # Add parameters for tuning the perception algorithms\r\n            {'detection_threshold': 0.5},\r\n            {'tracking_lifetime': 2.0},\r\n            {'map_resolution': 0.05},  # meters per cell\r\n            {'max_map_size_x': 20.0},  # meters\r\n            {'max_map_size_y': 20.0},  # meters\r\n        ],\r\n        remappings=[\r\n            ('/camera/image_raw', '/front_camera/image_raw'),\r\n            ('/camera/camera_info', '/front_camera/camera_info'),\r\n            ('/perception/detections', '/isaac/detections'),\r\n            ('/vslam/odometry', '/isaac/vslam_odom'),\r\n        ],\r\n        output='screen'\r\n    )\r\n    \r\n    return LaunchDescription([\r\n        namespace_arg,\r\n        perception_node\r\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-implement-a-reinforcement-learning-component",children:"Step 3: Implement a Reinforcement Learning Component"}),"\n",(0,t.jsx)(n.p,{children:"For the AI-robot brain integration, let's implement a simple navigation policy using reinforcement learning:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Create ",(0,t.jsx)(n.code,{children:"perception_pipeline/perception_pipeline/navigation_rl.py"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom nav_msgs.msg import Odometry\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom std_msgs.msg import Float32\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport random\r\nfrom collections import deque\r\nimport math\r\n\r\n\r\nclass NavigationPolicyNetwork(nn.Module):\r\n    """Simple neural network for navigation policy"""\r\n    def __init__(self, state_size=24, action_size=5):\r\n        super(NavigationPolicyNetwork, self).__init__()\r\n        self.fc1 = nn.Linear(state_size, 128)\r\n        self.fc2 = nn.Linear(128, 128)\r\n        self.fc3 = nn.Linear(128, action_size)\r\n        self.relu = nn.ReLU()\r\n        \r\n    def forward(self, x):\r\n        x = self.relu(self.fc1(x))\r\n        x = self.relu(self.fc2(x))\r\n        return self.fc3(x)\r\n\r\n\r\nclass IsaacNavigationRL(Node):\r\n    """Reinforcement learning node for navigation"""\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'isaac_navigation_rl\')\r\n        \r\n        # Initialize neural networks\r\n        state_size = 24  # 20 LIDAR bins + 3 for goal direction + 1 for current speed\r\n        action_size = 5  # Discrete actions for navigation\r\n        \r\n        self.q_network = NavigationPolicyNetwork(state_size, action_size)\r\n        self.target_network = NavigationPolicyNetwork(state_size, action_size)\r\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\r\n        \r\n        # Initialize DQN parameters\r\n        self.memory = deque(maxlen=10000)\r\n        self.epsilon = 1.0  # Exploration rate\r\n        self.epsilon_min = 0.01\r\n        self.epsilon_decay = 0.995\r\n        self.gamma = 0.95  # Discount factor\r\n        self.tau = 0.005  # Target network update rate\r\n        self.batch_size = 32\r\n        \r\n        # Initialize state variables\r\n        self.laser_data = None\r\n        self.odom_data = None\r\n        self.goal_pose = None\r\n        self.current_action = None\r\n        \r\n        # Subscribers\r\n        self.laser_sub = self.create_subscription(\r\n            LaserScan,\r\n            \'/scan\',\r\n            self.laser_callback,\r\n            10\r\n        )\r\n        \r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            \'/odom\',\r\n            self.odom_callback,\r\n            10\r\n        )\r\n        \r\n        self.vslam_sub = self.create_subscription(\r\n            Odometry,\r\n            \'/vslam/odometry\',\r\n            self.vslam_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.episode_reward_pub = self.create_publisher(Float32, \'/rl/episode_reward\', 10)\r\n        \r\n        # Timer for control loop\r\n        self.control_timer = self.create_timer(0.1, self.control_step)\r\n        \r\n        self.get_logger().info("Isaac Navigation RL node initialized")\r\n\r\n    def laser_callback(self, msg):\r\n        """Process LIDAR data"""\r\n        # Simplify LIDAR data to fixed-size representation\r\n        ranges = np.array(msg.ranges)\r\n        ranges = np.clip(ranges, msg.range_min, msg.range_max)\r\n        \r\n        # Bin the data into fixed number of directions\r\n        bin_count = 20\r\n        step = len(ranges) // bin_count\r\n        binned_ranges = [np.min(ranges[i:i+step]) for i in range(0, len(ranges), step)]\r\n        binned_ranges = binned_ranges[:bin_count]\r\n        \r\n        self.laser_data = np.array(binned_ranges)\r\n\r\n    def odom_callback(self, msg):\r\n        """Process odometry data"""\r\n        self.odom_data = msg\r\n\r\n    def vslam_callback(self, msg):\r\n        """Process VSLAM data"""\r\n        # Use VSLAM pose estimation if available\r\n        self.vslam_data = msg\r\n\r\n    def get_state(self):\r\n        """Construct state representation from sensor data"""\r\n        if self.laser_data is None or self.odom_data is None:\r\n            return None\r\n            \r\n        # Combine LIDAR data, goal direction, and current speed\r\n        state = self.laser_data.copy()\r\n        \r\n        # Add goal direction if available\r\n        if self.goal_pose is not None and self.odom_data is not None:\r\n            dx = self.goal_pose.pose.position.x - self.odom_data.pose.pose.position.x\r\n            dy = self.goal_pose.pose.position.y - self.odom_data.pose.pose.position.y\r\n            dist_to_goal = math.sqrt(dx**2 + dy**2)\r\n            angle_to_goal = math.atan2(dy, dx)\r\n            \r\n            state = np.append(state, [dx, dy, dist_to_goal])\r\n        else:\r\n            # Default values if no goal\r\n            state = np.append(state, [0.0, 0.0, 10.0])\r\n        \r\n        # Add current linear/angular velocities\r\n        lin_vel = self.odom_data.twist.twist.linear.x\r\n        ang_vel = self.odom_data.twist.twist.angular.z\r\n        state = np.append(state, [lin_vel])\r\n        \r\n        return state\r\n\r\n    def select_action(self, state):\r\n        """Select action using epsilon-greedy policy"""\r\n        if random.random() < self.epsilon:\r\n            # Random action for exploration\r\n            return random.randrange(5)\r\n        \r\n        # Use neural network for exploitation\r\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\r\n        q_values = self.q_network(state_tensor)\r\n        return q_values.argmax().item()\r\n\r\n    def step_simulation(self):\r\n        """Simulate a step in the environment (placeholder)"""\r\n        # In a real implementation, this would interface with the simulator or real robot\r\n        # For now, we\'ll just return a mock reward\r\n        \r\n        # Calculate reward based on:\r\n        # 1. Distance to goal (positive reward for getting closer)\r\n        # 2. Collision avoidance (negative reward for obstacles too close)\r\n        # 3. Efficiency (small negative reward for taking too long)\r\n        \r\n        reward = -0.01  # Small negative reward for time\r\n        \r\n        if self.laser_data is not None:\r\n            # Check for potential collisions\r\n            min_distance = np.min(self.laser_data)\r\n            if min_distance < 0.5:  # Threshold for collision danger\r\n                reward -= 1.0  # Significant negative reward for collision risk\r\n            \r\n            # Positive reward for forward progress\r\n            if self.odom_data is not None:\r\n                lin_vel = self.odom_data.twist.twist.linear.x\r\n                if lin_vel > 0.1:  # Moving forward\r\n                    reward += 0.05\r\n        \r\n        return reward, False  # No terminal condition for now\r\n\r\n    def remember(self, state, action, reward, next_state, done):\r\n        """Store experience in replay memory"""\r\n        self.memory.append((state, action, reward, next_state, done))\r\n\r\n    def replay(self):\r\n        """Train the model on a batch of experiences"""\r\n        if len(self.memory) < self.batch_size:\r\n            return\r\n            \r\n        batch = random.sample(self.memory, self.batch_size)\r\n        states = torch.FloatTensor([e[0] for e in batch])\r\n        actions = torch.LongTensor([e[1] for e in batch])\r\n        rewards = torch.FloatTensor([e[2] for e in batch])\r\n        next_states = torch.FloatTensor([e[3] for e in batch])\r\n        dones = torch.BoolTensor([e[4] for e in batch])\r\n        \r\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\r\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\r\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\r\n        \r\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\r\n        \r\n        self.optimizer.zero_grad()\r\n        loss.backward()\r\n        self.optimizer.step()\r\n\r\n    def update_target_network(self):\r\n        """Soft update the target network"""\r\n        for target_param, local_param in zip(\r\n            self.target_network.parameters(), \r\n            self.q_network.parameters()\r\n        ):\r\n            target_param.data.copy_(\r\n                self.tau * local_param.data + (1.0 - self.tau) * target_param.data\r\n            )\r\n\r\n    def control_step(self):\r\n        """Main control loop for RL navigation"""\r\n        state = self.get_state()\r\n        if state is None:\r\n            return\r\n            \r\n        # Select action using current policy\r\n        action = self.select_action(state)\r\n        \r\n        # Convert discrete action to continuous velocity commands\r\n        cmd_vel = Twist()\r\n        if action == 0:  # Move forward\r\n            cmd_vel.linear.x = 0.3\r\n            cmd_vel.angular.z = 0.0\r\n        elif action == 1:  # Turn left\r\n            cmd_vel.linear.x = 0.1\r\n            cmd_vel.angular.z = 0.3\r\n        elif action == 2:  # Turn right\r\n            cmd_vel.linear.x = 0.1\r\n            cmd_vel.angular.z = -0.3\r\n        elif action == 3:  # Move backward\r\n            cmd_vel.linear.x = -0.1\r\n            cmd_vel.angular.z = 0.0\r\n        else:  # Stop\r\n            cmd_vel.linear.x = 0.0\r\n            cmd_vel.angular.z = 0.0\r\n        \r\n        # Publish command\r\n        self.cmd_pub.publish(cmd_vel)\r\n        \r\n        # Get reward and update model\r\n        reward, done = self.step_simulation()\r\n        \r\n        next_state = self.get_state()\r\n        if next_state is not None:\r\n            self.remember(state, action, reward, next_state, done)\r\n        \r\n        # Train the model\r\n        if len(self.memory) > self.batch_size:\r\n            self.replay()\r\n            self.update_target_network()\r\n        \r\n        # Decay exploration rate\r\n        if self.epsilon > self.epsilon_min:\r\n            self.epsilon *= self.epsilon_decay\r\n        \r\n        # Publish episode reward if needed\r\n        reward_msg = Float32()\r\n        reward_msg.data = reward\r\n        self.episode_reward_pub.publish(reward_msg)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    \r\n    rl_node = IsaacNavigationRL()\r\n    \r\n    try:\r\n        rclpy.spin(rl_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        rl_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-running-the-complete-system",children:"Step 4: Running the Complete System"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Build your new package:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ws\r\nsource install/setup.bash\r\ncolcon build --packages-select perception_pipeline\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Launch the complete perception and navigation system:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\r\nros2 launch perception_pipeline perception_pipeline_launch.py\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"In another terminal, launch the RL navigation:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\r\nros2 run perception_pipeline navigation_rl.py\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-5-testing-and-evaluation",children:"Step 5: Testing and Evaluation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Monitor the perception output:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# View detection results\r\nros2 topic echo /perception/detections\r\n\r\n# View VSLAM odometry\r\nros2 topic echo /vslam/odometry\r\n\r\n# View robot commands\r\nros2 topic echo /cmd_vel\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["Evaluate the performance of your perception system by:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Checking detection accuracy in various scenarios"}),"\n",(0,t.jsx)(n.li,{children:"Measuring pose estimation drift over time"}),"\n",(0,t.jsx)(n.li,{children:"Assessing navigation performance in different environments"}),"\n",(0,t.jsx)(n.li,{children:"Monitoring resource usage (CPU, GPU, memory)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"expected-results",children:"Expected Results"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The perception system should detect objects in the camera feed"}),"\n",(0,t.jsx)(n.li,{children:"The VSLAM system should estimate robot motion relative to the environment"}),"\n",(0,t.jsx)(n.li,{children:"The RL navigation system should move the robot toward goals while avoiding obstacles"}),"\n",(0,t.jsx)(n.li,{children:"The system should run efficiently on your NVIDIA hardware platform"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"If GPU acceleration isn't working, verify that CUDA and TensorRT are properly installed"}),"\n",(0,t.jsx)(n.li,{children:"If detections are inaccurate, try calibrating your camera and adjusting thresholds"}),"\n",(0,t.jsx)(n.li,{children:"If VSLAM is unstable, check that the robot has sufficient distinctive visual features in its environment"}),"\n",(0,t.jsx)(n.li,{children:"If navigation is erratic, tune the RL hyperparameters (epsilon decay, learning rate, etc.)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"extension-activities",children:"Extension Activities"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Train a custom object detection model for your specific application"}),"\n",(0,t.jsx)(n.li,{children:"Implement a more sophisticated navigation policy using advanced RL algorithms (PPO, SAC)"}),"\n",(0,t.jsx)(n.li,{children:"Add semantic mapping capability to create labeled environment maps"}),"\n",(0,t.jsx)(n.li,{children:"Integrate IMU data for improved motion estimation"}),"\n",(0,t.jsx)(n.li,{children:"Optimize the pipeline for your specific robot's computational constraints"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"In this lab, you've implemented a complete perception system using NVIDIA Isaac tools, including visual SLAM for localization and a reinforcement learning algorithm for navigation. This demonstrates the tight integration between AI perception and robot control that is possible with NVIDIA's platform."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var i=r(6540);const t={},a=i.createContext(t);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:n},e.children)}},9161:(e,n,r)=>{r.d(n,{A:()=>i});const i="data:image/png;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNjAwIj48cmVjdCB3aWR0aD0iODAwIiBoZWlnaHQ9IjYwMCIgZmlsbD0iI2U4ZjRmZCIvPjx0ZXh0IHg9IjQwMCIgeT0iMzAwIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iMjQiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiMxYTczZTgiPk5WSURJQSBJc2FhYyBQZXJjZXB0aW9uIFBpcGVsaW5lPC90ZXh0Pjx0ZXh0IHg9IjQwMCIgeT0iMzUwIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iMTYiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiM1ZjYzNjgiPkFJLWRyaXZlbiBwZXJjZXB0aW9uIGZvciByb2JvdGljcyB1c2luZyBJc2FhYyBwbGF0Zm9ybTwvdGV4dD48L3N2Zz4="}}]);